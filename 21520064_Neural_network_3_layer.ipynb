{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Tải dữ liệu** "
      ],
      "metadata": {
        "id": "8HRKj4XA5WMC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yK7KUNP4AmW5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f0a0497-0188-4bfd-8e09-693eb2564ec7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import thư viện cần thiết\n"
      ],
      "metadata": {
        "id": "DlC3OM8f5iXx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CtnQpeohAORp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "from random import randint\n",
        "from google.colab.patches import cv2_imshow\n",
        "np.random.seed()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quan sát dữ liệu"
      ],
      "metadata": {
        "id": "VWlDXuBC5oIL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "naKReMwCBXQg"
      },
      "outputs": [],
      "source": [
        "x_train = X_train.reshape(len(X_train), 28*28)\n",
        "x_test = X_test.reshape(len(X_test), 28*28)\n",
        "x_train = x_train / 255.\n",
        "x_test = x_test / 255."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv2_imshow(X_train[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 45
        },
        "id": "Jff_S6le5sdy",
        "outputId": "f9075c74-263e-4fda-9573-9daee60fcbf5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=28x28 at 0x7F8C174F5750>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA/0lEQVR4nGNgGHhgPP/vfCMccgbv/vz58xa7nNnjv3/ev/xjyYYpxWXz4M/fP6dC/vytgggwIUnOPCDDwMBgxHOQQRdD0tibkfFQKeOL85OYGLG5ZTOPd6UoA8Pfz2gOVlv69+WFEAj775+lKHLsm/58cBeWgUkeRpG0/PPHHs5Blzz2dx+C8//vEWTX+hj834SQ/Pf/ArLG0D/PJOHWt//dxYMqeR8u1/znoTsDquREKMtg6Z+1DKgg7O9DCKPo3d9FaHIMoX9+TjKQDd308O/95RaYkn/+PL3+58+fI03oUgwMMsf//Pn758/LiZhSDAwMkg1//v7pVcUqR1cAAKxwbkTVIzd2AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I833KVsly4_h",
        "outputId": "e9a81658-6603-4123-cdd5-24abc7cdbb3e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(60000,)"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XP4I7Iptzt4_",
        "outputId": "ea861bdc-6c06-4215-9c19-bb0cb4d9fa7e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "x_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBpEe1vWz3SH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc0a5ccc-43e9-4ddb-b9e4-4ab5efc49b29"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
              "       0.07058824, 0.49411765, 0.53333333, 0.68627451, 0.10196078,\n",
              "       0.65098039, 1.        , 0.96862745, 0.49803922, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.11764706, 0.14117647, 0.36862745, 0.60392157,\n",
              "       0.66666667, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
              "       0.99215686, 0.88235294, 0.6745098 , 0.99215686, 0.94901961,\n",
              "       0.76470588, 0.25098039, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.19215686, 0.93333333,\n",
              "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
              "       0.99215686, 0.99215686, 0.99215686, 0.98431373, 0.36470588,\n",
              "       0.32156863, 0.32156863, 0.21960784, 0.15294118, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.07058824, 0.85882353, 0.99215686, 0.99215686,\n",
              "       0.99215686, 0.99215686, 0.99215686, 0.77647059, 0.71372549,\n",
              "       0.96862745, 0.94509804, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.31372549, 0.61176471, 0.41960784, 0.99215686, 0.99215686,\n",
              "       0.80392157, 0.04313725, 0.        , 0.16862745, 0.60392157,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
              "       0.00392157, 0.60392157, 0.99215686, 0.35294118, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.54509804,\n",
              "       0.99215686, 0.74509804, 0.00784314, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.04313725, 0.74509804, 0.99215686,\n",
              "       0.2745098 , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.1372549 , 0.94509804, 0.88235294, 0.62745098,\n",
              "       0.42352941, 0.00392157, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.31764706, 0.94117647, 0.99215686, 0.99215686, 0.46666667,\n",
              "       0.09803922, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.17647059,\n",
              "       0.72941176, 0.99215686, 0.99215686, 0.58823529, 0.10588235,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.0627451 , 0.36470588,\n",
              "       0.98823529, 0.99215686, 0.73333333, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.97647059, 0.99215686,\n",
              "       0.97647059, 0.25098039, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.18039216, 0.50980392,\n",
              "       0.71764706, 0.99215686, 0.99215686, 0.81176471, 0.00784314,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.15294118,\n",
              "       0.58039216, 0.89803922, 0.99215686, 0.99215686, 0.99215686,\n",
              "       0.98039216, 0.71372549, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.09411765, 0.44705882, 0.86666667, 0.99215686, 0.99215686,\n",
              "       0.99215686, 0.99215686, 0.78823529, 0.30588235, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.09019608, 0.25882353, 0.83529412, 0.99215686,\n",
              "       0.99215686, 0.99215686, 0.99215686, 0.77647059, 0.31764706,\n",
              "       0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.07058824, 0.67058824, 0.85882353,\n",
              "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.76470588,\n",
              "       0.31372549, 0.03529412, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.21568627, 0.6745098 ,\n",
              "       0.88627451, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
              "       0.95686275, 0.52156863, 0.04313725, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.53333333, 0.99215686, 0.99215686, 0.99215686,\n",
              "       0.83137255, 0.52941176, 0.51764706, 0.0627451 , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "x_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wQHkYL_Emne",
        "outputId": "aa55ca7a-fb4b-44a4-e16c-6405f02a8b1e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "y_train[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tạo model\n"
      ],
      "metadata": {
        "id": "e_ZQsf9S5x0d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-SA-qIyMakYQ"
      },
      "outputs": [],
      "source": [
        "class NN:\n",
        "    first_layer = {}\n",
        "    second_layer = {}\n",
        "\n",
        "    def __init__(self, inputs, hidden, outputs):\n",
        "        self.first_layer['weight'] = np.random.randn(inputs, hidden) / np.sqrt(inputs)\n",
        "        self.first_layer['bias'] = np.random.randn(hidden, 1) / np.sqrt(hidden)\n",
        "        self.second_layer['weight'] = np.random.randn(hidden, outputs) / np.sqrt(hidden)\n",
        "        self.second_layer['bias'] = np.random.randn(outputs, 1) / np.sqrt(outputs)\n",
        "      \n",
        "        self.input_size = inputs\n",
        "        self.hid_size = hidden\n",
        "        self.output_size = outputs\n",
        "\n",
        "    def activfunc(self, Z, type = 'ReLU', deri = False):\n",
        "        if type == 'ReLU':\n",
        "            if deri == True: # neu co dao ham\n",
        "                return np.array([1 if i > 0 else 0 for i in np.squeeze(Z)])\n",
        "            else:\n",
        "                return np.array([i if i > 0 else 0 for i in np.squeeze(Z)])\n",
        "        elif type == 'Sigmoid':\n",
        "            if deri == True:\n",
        "                return 1/(1+np.exp(-Z))*(1-1/(1+np.exp(-Z)))\n",
        "            else:\n",
        "                return 1/(1+np.exp(-Z))\n",
        "        elif type == 'tanh':\n",
        "            if deri == True:\n",
        "                return \n",
        "            else:\n",
        "                return 1-(np.tanh(Z))**2\n",
        "        else:\n",
        "            raise TypeError('Invalid type!')\n",
        "\n",
        "    def Softmax(self,z):\n",
        "        return 1/sum(np.exp(z)) * np.exp(z)\n",
        "\n",
        "    def cross_entropy_error(self, y_hat, y):\n",
        "        return -np.log(y_hat[y])\n",
        "\n",
        "    def feedforward(self,x,y):\n",
        "        Z1 = np.dot(self.first_layer['weight'].T, x).reshape((self.hid_size,1)) + self.first_layer['bias'] # Z[1] = W[1] * X + b[1]\n",
        "        A1 = np.array(self.activfunc(Z1)).reshape((self.hid_size,1)) # A[1] = sigmoid(Z[1])\n",
        "        Z2 = np.dot(self.second_layer['weight'].T, A1).reshape((self.output_size,1)) + self.second_layer['bias'] # Z[2] = W[2] * A[1] + b[2]\n",
        "        y_hat = np.squeeze(self.Softmax(Z2))\n",
        "        error = self.cross_entropy_error(y_hat,y)\n",
        "        para = {\n",
        "            'Z1': Z1,\n",
        "            'A1': A1,\n",
        "            'Z2': Z2,\n",
        "            'y_hat': y_hat,\n",
        "            'error': error\n",
        "        }\n",
        "        return para\n",
        "\n",
        "    def predict(self, x):\n",
        "        return np.argmax(self.feedforward(x, 1)['y_hat'])\n",
        "\n",
        "    def back_propagation(self, x, y, f_result):\n",
        "        # onehot encoder cho y\n",
        "        E = np.array([0]*self.output_size).reshape((1, self.output_size))\n",
        "        E[0][y] = 1\n",
        "        dtmp = (f_result['y_hat'] - E).reshape((self.output_size,1)) \n",
        "        db2 = dtmp\n",
        "        dW2 = np.dot(dtmp, f_result['A1'].T)\n",
        "        delta = np.dot(self.second_layer['weight'], dtmp) * self.activfunc(f_result['Z1'], deri = True).reshape(self.hid_size, 1)\n",
        "        db1 = delta\n",
        "        dW1 = np.dot(db1.reshape((self.hid_size, 1)),x.reshape((1, 784)))\n",
        "\n",
        "        grad = {\n",
        "            'dW2':dW2,\n",
        "            'db2':db2,\n",
        "            'db1':db1,\n",
        "            'dW1':dW1\n",
        "        }\n",
        "        return grad\n",
        "\n",
        "    def optimize(self,b_result, learning_rate):\n",
        "        self.second_layer['weight'] -= learning_rate*b_result['dW2'].T\n",
        "        self.second_layer['bias'] -= learning_rate*b_result['db2']\n",
        "        self.first_layer['weight'] -= learning_rate*b_result['dW1'].T\n",
        "        self.first_layer['bias'] -= learning_rate*b_result['db1']\n",
        "\n",
        "\n",
        "    def loss(self,X_train,Y_train):\n",
        "        loss = 0\n",
        "        for n in range(len(X_train)):\n",
        "            y = Y_train[n]\n",
        "            x = X_train[n][:]\n",
        "            loss += self.feedforward(x,y)['error']\n",
        "        return loss / len(X_train)\n",
        "\n",
        "    def train(self, X_train, Y_train, num_iterations = 1000, learning_rate = 0.5):\n",
        "        # generate random list index for train\n",
        "        rand_indices = np.random.choice(len(X_train), num_iterations, replace=True)\n",
        "        \n",
        "        def l_rate(base_rate, ite, num_iterations, schedule = False):\n",
        "        # determine whether to use the learning schedule\n",
        "            if schedule == True:\n",
        "                return base_rate * 10 ** (-np.floor(ite/num_iterations*5))\n",
        "            else:\n",
        "                return base_rate\n",
        "\n",
        "        count = 1\n",
        "        loss_dict = {}\n",
        "        test_dict = {}\n",
        "        num_epochs = 1\n",
        "        for i in rand_indices:\n",
        "            f_result = self.feedforward(X_train[i], Y_train[i])\n",
        "            b_result = self.back_propagation(X_train[i], Y_train[i], f_result)\n",
        "            self.optimize(b_result, l_rate(learning_rate, i, num_iterations, True))\n",
        "            \n",
        "            if count % 1000 == 0:\n",
        "                print('Trained for {} times,'.format(count))\n",
        "                if count % 5000 == 0:\n",
        "                    loss = self.loss(X_train,Y_train)\n",
        "                    test, wrong_case = self.testing(x_test,y_test)\n",
        "                    print('Trained for {} epoch(s),'.format(num_epochs),'loss = {}, test = {}'.format(loss,test))\n",
        "                    loss_dict[str(count)]=loss\n",
        "                    test_dict[str(count)]=test\n",
        "                    num_epochs += 1\n",
        "                    \n",
        "            count += 1\n",
        "\n",
        "        print('Training finished!')\n",
        "        return loss_dict, test_dict\n",
        "\n",
        "    def testing(self, X_test, y_test):\n",
        "        wrong_case = []\n",
        "        total_correct = 0\n",
        "        for n in range(len(X_test)):\n",
        "            y = y_test[n]\n",
        "            x = X_test[n][:]\n",
        "            prediction = np.argmax(self.feedforward(x, y)['y_hat'])\n",
        "            if (prediction == y):\n",
        "                total_correct += 1\n",
        "            else: wrong_case.append(n)\n",
        "        print('Accuarcy Test: ',total_correct / len(X_test))\n",
        "        return total_correct/np.float(len(X_test)), wrong_case"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test thử các trường hợp khác nhau của hidden size"
      ],
      "metadata": {
        "id": "iZYToTdR51xY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_iterations = 200000\n",
        "learning_rate = 0.01\n",
        "num_inputs = 28*28\n",
        "num_outputs = 10\n",
        "hidden_size = [32, 64, 128, 256, 512]\n",
        "acc_per_hidden_size = []\n",
        "# data fitting, training and accuracy evaluation\n",
        "for i in hidden_size:\n",
        "  model = NN(num_inputs, i, num_outputs)\n",
        "  cost_dict, tests_dict = model.train(x_train, y_train, num_iterations=num_iterations, learning_rate=learning_rate)\n",
        "  tmp, _ = model.testing(x_test, y_test)\n",
        "  acc_per_hidden_size.append(tmp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgVuptBGtRLC",
        "outputId": "391c6b9d-355c-45df-80b4-39fa4457aac4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trained for 1000 times,\n",
            "Trained for 2000 times,\n",
            "Trained for 3000 times,\n",
            "Trained for 4000 times,\n",
            "Trained for 5000 times,\n",
            "Accuarcy Test:  0.8736\n",
            "Trained for 1 epoch(s), loss = 0.4210339050504755, test = 0.8736\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:138: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trained for 6000 times,\n",
            "Trained for 7000 times,\n",
            "Trained for 8000 times,\n",
            "Trained for 9000 times,\n",
            "Trained for 10000 times,\n",
            "Accuarcy Test:  0.8734\n",
            "Trained for 2 epoch(s), loss = 0.40856080867280514, test = 0.8734\n",
            "Trained for 11000 times,\n",
            "Trained for 12000 times,\n",
            "Trained for 13000 times,\n",
            "Trained for 14000 times,\n",
            "Trained for 15000 times,\n",
            "Accuarcy Test:  0.903\n",
            "Trained for 3 epoch(s), loss = 0.3184322989365496, test = 0.903\n",
            "Trained for 16000 times,\n",
            "Trained for 17000 times,\n",
            "Trained for 18000 times,\n",
            "Trained for 19000 times,\n",
            "Trained for 20000 times,\n",
            "Accuarcy Test:  0.9095\n",
            "Trained for 4 epoch(s), loss = 0.2984971236739726, test = 0.9095\n",
            "Trained for 21000 times,\n",
            "Trained for 22000 times,\n",
            "Trained for 23000 times,\n",
            "Trained for 24000 times,\n",
            "Trained for 25000 times,\n",
            "Accuarcy Test:  0.9237\n",
            "Trained for 5 epoch(s), loss = 0.26708685518210024, test = 0.9237\n",
            "Trained for 26000 times,\n",
            "Trained for 27000 times,\n",
            "Trained for 28000 times,\n",
            "Trained for 29000 times,\n",
            "Trained for 30000 times,\n",
            "Accuarcy Test:  0.9193\n",
            "Trained for 6 epoch(s), loss = 0.2685633528772881, test = 0.9193\n",
            "Trained for 31000 times,\n",
            "Trained for 32000 times,\n",
            "Trained for 33000 times,\n",
            "Trained for 34000 times,\n",
            "Trained for 35000 times,\n",
            "Accuarcy Test:  0.9339\n",
            "Trained for 7 epoch(s), loss = 0.21887818419697175, test = 0.9339\n",
            "Trained for 36000 times,\n",
            "Trained for 37000 times,\n",
            "Trained for 38000 times,\n",
            "Trained for 39000 times,\n",
            "Trained for 40000 times,\n",
            "Accuarcy Test:  0.9348\n",
            "Trained for 8 epoch(s), loss = 0.2164050094327302, test = 0.9348\n",
            "Trained for 41000 times,\n",
            "Trained for 42000 times,\n",
            "Trained for 43000 times,\n",
            "Trained for 44000 times,\n",
            "Trained for 45000 times,\n",
            "Accuarcy Test:  0.9266\n",
            "Trained for 9 epoch(s), loss = 0.24409801343003812, test = 0.9266\n",
            "Trained for 46000 times,\n",
            "Trained for 47000 times,\n",
            "Trained for 48000 times,\n",
            "Trained for 49000 times,\n",
            "Trained for 50000 times,\n",
            "Accuarcy Test:  0.9392\n",
            "Trained for 10 epoch(s), loss = 0.19270041231769786, test = 0.9392\n",
            "Trained for 51000 times,\n",
            "Trained for 52000 times,\n",
            "Trained for 53000 times,\n",
            "Trained for 54000 times,\n",
            "Trained for 55000 times,\n",
            "Accuarcy Test:  0.9337\n",
            "Trained for 11 epoch(s), loss = 0.2135588658321638, test = 0.9337\n",
            "Trained for 56000 times,\n",
            "Trained for 57000 times,\n",
            "Trained for 58000 times,\n",
            "Trained for 59000 times,\n",
            "Trained for 60000 times,\n",
            "Accuarcy Test:  0.9432\n",
            "Trained for 12 epoch(s), loss = 0.18265251329459675, test = 0.9432\n",
            "Trained for 61000 times,\n",
            "Trained for 62000 times,\n",
            "Trained for 63000 times,\n",
            "Trained for 64000 times,\n",
            "Trained for 65000 times,\n",
            "Accuarcy Test:  0.9351\n",
            "Trained for 13 epoch(s), loss = 0.2051753114861407, test = 0.9351\n",
            "Trained for 66000 times,\n",
            "Trained for 67000 times,\n",
            "Trained for 68000 times,\n",
            "Trained for 69000 times,\n",
            "Trained for 70000 times,\n",
            "Accuarcy Test:  0.9396\n",
            "Trained for 14 epoch(s), loss = 0.18715575095489928, test = 0.9396\n",
            "Trained for 71000 times,\n",
            "Trained for 72000 times,\n",
            "Trained for 73000 times,\n",
            "Trained for 74000 times,\n",
            "Trained for 75000 times,\n",
            "Accuarcy Test:  0.9347\n",
            "Trained for 15 epoch(s), loss = 0.19998954909538683, test = 0.9347\n",
            "Trained for 76000 times,\n",
            "Trained for 77000 times,\n",
            "Trained for 78000 times,\n",
            "Trained for 79000 times,\n",
            "Trained for 80000 times,\n",
            "Accuarcy Test:  0.9476\n",
            "Trained for 16 epoch(s), loss = 0.16085875761057417, test = 0.9476\n",
            "Trained for 81000 times,\n",
            "Trained for 82000 times,\n",
            "Trained for 83000 times,\n",
            "Trained for 84000 times,\n",
            "Trained for 85000 times,\n",
            "Accuarcy Test:  0.9414\n",
            "Trained for 17 epoch(s), loss = 0.17786334391647168, test = 0.9414\n",
            "Trained for 86000 times,\n",
            "Trained for 87000 times,\n",
            "Trained for 88000 times,\n",
            "Trained for 89000 times,\n",
            "Trained for 90000 times,\n",
            "Accuarcy Test:  0.9487\n",
            "Trained for 18 epoch(s), loss = 0.15198341178676508, test = 0.9487\n",
            "Trained for 91000 times,\n",
            "Trained for 92000 times,\n",
            "Trained for 93000 times,\n",
            "Trained for 94000 times,\n",
            "Trained for 95000 times,\n",
            "Accuarcy Test:  0.9404\n",
            "Trained for 19 epoch(s), loss = 0.18044278964169985, test = 0.9404\n",
            "Trained for 96000 times,\n",
            "Trained for 97000 times,\n",
            "Trained for 98000 times,\n",
            "Trained for 99000 times,\n",
            "Trained for 100000 times,\n",
            "Accuarcy Test:  0.9472\n",
            "Trained for 20 epoch(s), loss = 0.15827655198385485, test = 0.9472\n",
            "Trained for 101000 times,\n",
            "Trained for 102000 times,\n",
            "Trained for 103000 times,\n",
            "Trained for 104000 times,\n",
            "Trained for 105000 times,\n",
            "Accuarcy Test:  0.95\n",
            "Trained for 21 epoch(s), loss = 0.14725869201184175, test = 0.95\n",
            "Trained for 106000 times,\n",
            "Trained for 107000 times,\n",
            "Trained for 108000 times,\n",
            "Trained for 109000 times,\n",
            "Trained for 110000 times,\n",
            "Accuarcy Test:  0.9433\n",
            "Trained for 22 epoch(s), loss = 0.16930075426149, test = 0.9433\n",
            "Trained for 111000 times,\n",
            "Trained for 112000 times,\n",
            "Trained for 113000 times,\n",
            "Trained for 114000 times,\n",
            "Trained for 115000 times,\n",
            "Accuarcy Test:  0.9463\n",
            "Trained for 23 epoch(s), loss = 0.15733521414348334, test = 0.9463\n",
            "Trained for 116000 times,\n",
            "Trained for 117000 times,\n",
            "Trained for 118000 times,\n",
            "Trained for 119000 times,\n",
            "Trained for 120000 times,\n",
            "Accuarcy Test:  0.9504\n",
            "Trained for 24 epoch(s), loss = 0.14764442620284746, test = 0.9504\n",
            "Trained for 121000 times,\n",
            "Trained for 122000 times,\n",
            "Trained for 123000 times,\n",
            "Trained for 124000 times,\n",
            "Trained for 125000 times,\n",
            "Accuarcy Test:  0.944\n",
            "Trained for 25 epoch(s), loss = 0.1643787762800017, test = 0.944\n",
            "Trained for 126000 times,\n",
            "Trained for 127000 times,\n",
            "Trained for 128000 times,\n",
            "Trained for 129000 times,\n",
            "Trained for 130000 times,\n",
            "Accuarcy Test:  0.9527\n",
            "Trained for 26 epoch(s), loss = 0.14200663235005645, test = 0.9527\n",
            "Trained for 131000 times,\n",
            "Trained for 132000 times,\n",
            "Trained for 133000 times,\n",
            "Trained for 134000 times,\n",
            "Trained for 135000 times,\n",
            "Accuarcy Test:  0.9534\n",
            "Trained for 27 epoch(s), loss = 0.14156017637978624, test = 0.9534\n",
            "Trained for 136000 times,\n",
            "Trained for 137000 times,\n",
            "Trained for 138000 times,\n",
            "Trained for 139000 times,\n",
            "Trained for 140000 times,\n",
            "Accuarcy Test:  0.9471\n",
            "Trained for 28 epoch(s), loss = 0.144530347459321, test = 0.9471\n",
            "Trained for 141000 times,\n",
            "Trained for 142000 times,\n",
            "Trained for 143000 times,\n",
            "Trained for 144000 times,\n",
            "Trained for 145000 times,\n",
            "Accuarcy Test:  0.9411\n",
            "Trained for 29 epoch(s), loss = 0.16756699638738098, test = 0.9411\n",
            "Trained for 146000 times,\n",
            "Trained for 147000 times,\n",
            "Trained for 148000 times,\n",
            "Trained for 149000 times,\n",
            "Trained for 150000 times,\n",
            "Accuarcy Test:  0.9563\n",
            "Trained for 30 epoch(s), loss = 0.12895573220613912, test = 0.9563\n",
            "Trained for 151000 times,\n",
            "Trained for 152000 times,\n",
            "Trained for 153000 times,\n",
            "Trained for 154000 times,\n",
            "Trained for 155000 times,\n",
            "Accuarcy Test:  0.9538\n",
            "Trained for 31 epoch(s), loss = 0.13588859137600598, test = 0.9538\n",
            "Trained for 156000 times,\n",
            "Trained for 157000 times,\n",
            "Trained for 158000 times,\n",
            "Trained for 159000 times,\n",
            "Trained for 160000 times,\n",
            "Accuarcy Test:  0.9514\n",
            "Trained for 32 epoch(s), loss = 0.14046543731896366, test = 0.9514\n",
            "Trained for 161000 times,\n",
            "Trained for 162000 times,\n",
            "Trained for 163000 times,\n",
            "Trained for 164000 times,\n",
            "Trained for 165000 times,\n",
            "Accuarcy Test:  0.9573\n",
            "Trained for 33 epoch(s), loss = 0.1234213884474188, test = 0.9573\n",
            "Trained for 166000 times,\n",
            "Trained for 167000 times,\n",
            "Trained for 168000 times,\n",
            "Trained for 169000 times,\n",
            "Trained for 170000 times,\n",
            "Accuarcy Test:  0.9574\n",
            "Trained for 34 epoch(s), loss = 0.1273287154663944, test = 0.9574\n",
            "Trained for 171000 times,\n",
            "Trained for 172000 times,\n",
            "Trained for 173000 times,\n",
            "Trained for 174000 times,\n",
            "Trained for 175000 times,\n",
            "Accuarcy Test:  0.9567\n",
            "Trained for 35 epoch(s), loss = 0.12919173856154573, test = 0.9567\n",
            "Trained for 176000 times,\n",
            "Trained for 177000 times,\n",
            "Trained for 178000 times,\n",
            "Trained for 179000 times,\n",
            "Trained for 180000 times,\n",
            "Accuarcy Test:  0.9592\n",
            "Trained for 36 epoch(s), loss = 0.13105766707732533, test = 0.9592\n",
            "Trained for 181000 times,\n",
            "Trained for 182000 times,\n",
            "Trained for 183000 times,\n",
            "Trained for 184000 times,\n",
            "Trained for 185000 times,\n",
            "Accuarcy Test:  0.9587\n",
            "Trained for 37 epoch(s), loss = 0.11715719105932505, test = 0.9587\n",
            "Trained for 186000 times,\n",
            "Trained for 187000 times,\n",
            "Trained for 188000 times,\n",
            "Trained for 189000 times,\n",
            "Trained for 190000 times,\n",
            "Accuarcy Test:  0.9607\n",
            "Trained for 38 epoch(s), loss = 0.11679470695798126, test = 0.9607\n",
            "Trained for 191000 times,\n",
            "Trained for 192000 times,\n",
            "Trained for 193000 times,\n",
            "Trained for 194000 times,\n",
            "Trained for 195000 times,\n",
            "Accuarcy Test:  0.9507\n",
            "Trained for 39 epoch(s), loss = 0.13890494317841245, test = 0.9507\n",
            "Trained for 196000 times,\n",
            "Trained for 197000 times,\n",
            "Trained for 198000 times,\n",
            "Trained for 199000 times,\n",
            "Trained for 200000 times,\n",
            "Accuarcy Test:  0.9587\n",
            "Trained for 40 epoch(s), loss = 0.1178466714204564, test = 0.9587\n",
            "Training finished!\n",
            "Accuarcy Test:  0.9587\n",
            "Trained for 1000 times,\n",
            "Trained for 2000 times,\n",
            "Trained for 3000 times,\n",
            "Trained for 4000 times,\n",
            "Trained for 5000 times,\n",
            "Accuarcy Test:  0.8878\n",
            "Trained for 1 epoch(s), loss = 0.3962265127892577, test = 0.8878\n",
            "Trained for 6000 times,\n",
            "Trained for 7000 times,\n",
            "Trained for 8000 times,\n",
            "Trained for 9000 times,\n",
            "Trained for 10000 times,\n",
            "Accuarcy Test:  0.8926\n",
            "Trained for 2 epoch(s), loss = 0.3532732387393824, test = 0.8926\n",
            "Trained for 11000 times,\n",
            "Trained for 12000 times,\n",
            "Trained for 13000 times,\n",
            "Trained for 14000 times,\n",
            "Trained for 15000 times,\n",
            "Accuarcy Test:  0.905\n",
            "Trained for 3 epoch(s), loss = 0.3138167543896614, test = 0.905\n",
            "Trained for 16000 times,\n",
            "Trained for 17000 times,\n",
            "Trained for 18000 times,\n",
            "Trained for 19000 times,\n",
            "Trained for 20000 times,\n",
            "Accuarcy Test:  0.9263\n",
            "Trained for 4 epoch(s), loss = 0.24045741246949878, test = 0.9263\n",
            "Trained for 21000 times,\n",
            "Trained for 22000 times,\n",
            "Trained for 23000 times,\n",
            "Trained for 24000 times,\n",
            "Trained for 25000 times,\n",
            "Accuarcy Test:  0.9305\n",
            "Trained for 5 epoch(s), loss = 0.22898558421916482, test = 0.9305\n",
            "Trained for 26000 times,\n",
            "Trained for 27000 times,\n",
            "Trained for 28000 times,\n",
            "Trained for 29000 times,\n",
            "Trained for 30000 times,\n",
            "Accuarcy Test:  0.9309\n",
            "Trained for 6 epoch(s), loss = 0.22564707886869156, test = 0.9309\n",
            "Trained for 31000 times,\n",
            "Trained for 32000 times,\n",
            "Trained for 33000 times,\n",
            "Trained for 34000 times,\n",
            "Trained for 35000 times,\n",
            "Accuarcy Test:  0.9323\n",
            "Trained for 7 epoch(s), loss = 0.2218514721648092, test = 0.9323\n",
            "Trained for 36000 times,\n",
            "Trained for 37000 times,\n",
            "Trained for 38000 times,\n",
            "Trained for 39000 times,\n",
            "Trained for 40000 times,\n",
            "Accuarcy Test:  0.9504\n",
            "Trained for 8 epoch(s), loss = 0.1638002872656033, test = 0.9504\n",
            "Trained for 41000 times,\n",
            "Trained for 42000 times,\n",
            "Trained for 43000 times,\n",
            "Trained for 44000 times,\n",
            "Trained for 45000 times,\n",
            "Accuarcy Test:  0.9474\n",
            "Trained for 9 epoch(s), loss = 0.17070687247946267, test = 0.9474\n",
            "Trained for 46000 times,\n",
            "Trained for 47000 times,\n",
            "Trained for 48000 times,\n",
            "Trained for 49000 times,\n",
            "Trained for 50000 times,\n",
            "Accuarcy Test:  0.9532\n",
            "Trained for 10 epoch(s), loss = 0.15910184513327477, test = 0.9532\n",
            "Trained for 51000 times,\n",
            "Trained for 52000 times,\n",
            "Trained for 53000 times,\n",
            "Trained for 54000 times,\n",
            "Trained for 55000 times,\n",
            "Accuarcy Test:  0.9525\n",
            "Trained for 11 epoch(s), loss = 0.14948535153142686, test = 0.9525\n",
            "Trained for 56000 times,\n",
            "Trained for 57000 times,\n",
            "Trained for 58000 times,\n",
            "Trained for 59000 times,\n",
            "Trained for 60000 times,\n",
            "Accuarcy Test:  0.9554\n",
            "Trained for 12 epoch(s), loss = 0.13554171658817296, test = 0.9554\n",
            "Trained for 61000 times,\n",
            "Trained for 62000 times,\n",
            "Trained for 63000 times,\n",
            "Trained for 64000 times,\n",
            "Trained for 65000 times,\n",
            "Accuarcy Test:  0.9563\n",
            "Trained for 13 epoch(s), loss = 0.1385927361030719, test = 0.9563\n",
            "Trained for 66000 times,\n",
            "Trained for 67000 times,\n",
            "Trained for 68000 times,\n",
            "Trained for 69000 times,\n",
            "Trained for 70000 times,\n",
            "Accuarcy Test:  0.9551\n",
            "Trained for 14 epoch(s), loss = 0.14075249799309203, test = 0.9551\n",
            "Trained for 71000 times,\n",
            "Trained for 72000 times,\n",
            "Trained for 73000 times,\n",
            "Trained for 74000 times,\n",
            "Trained for 75000 times,\n",
            "Accuarcy Test:  0.9577\n",
            "Trained for 15 epoch(s), loss = 0.12997330787711972, test = 0.9577\n",
            "Trained for 76000 times,\n",
            "Trained for 77000 times,\n",
            "Trained for 78000 times,\n",
            "Trained for 79000 times,\n",
            "Trained for 80000 times,\n",
            "Accuarcy Test:  0.9597\n",
            "Trained for 16 epoch(s), loss = 0.12766112897916235, test = 0.9597\n",
            "Trained for 81000 times,\n",
            "Trained for 82000 times,\n",
            "Trained for 83000 times,\n",
            "Trained for 84000 times,\n",
            "Trained for 85000 times,\n",
            "Accuarcy Test:  0.9602\n",
            "Trained for 17 epoch(s), loss = 0.12202914489387218, test = 0.9602\n",
            "Trained for 86000 times,\n",
            "Trained for 87000 times,\n",
            "Trained for 88000 times,\n",
            "Trained for 89000 times,\n",
            "Trained for 90000 times,\n",
            "Accuarcy Test:  0.9631\n",
            "Trained for 18 epoch(s), loss = 0.11335286153596329, test = 0.9631\n",
            "Trained for 91000 times,\n",
            "Trained for 92000 times,\n",
            "Trained for 93000 times,\n",
            "Trained for 94000 times,\n",
            "Trained for 95000 times,\n",
            "Accuarcy Test:  0.9497\n",
            "Trained for 19 epoch(s), loss = 0.1550887877635267, test = 0.9497\n",
            "Trained for 96000 times,\n",
            "Trained for 97000 times,\n",
            "Trained for 98000 times,\n",
            "Trained for 99000 times,\n",
            "Trained for 100000 times,\n",
            "Accuarcy Test:  0.9625\n",
            "Trained for 20 epoch(s), loss = 0.11174043607373356, test = 0.9625\n",
            "Trained for 101000 times,\n",
            "Trained for 102000 times,\n",
            "Trained for 103000 times,\n",
            "Trained for 104000 times,\n",
            "Trained for 105000 times,\n",
            "Accuarcy Test:  0.9615\n",
            "Trained for 21 epoch(s), loss = 0.11662710165125716, test = 0.9615\n",
            "Trained for 106000 times,\n",
            "Trained for 107000 times,\n",
            "Trained for 108000 times,\n",
            "Trained for 109000 times,\n",
            "Trained for 110000 times,\n",
            "Accuarcy Test:  0.9612\n",
            "Trained for 22 epoch(s), loss = 0.10615281535621818, test = 0.9612\n",
            "Trained for 111000 times,\n",
            "Trained for 112000 times,\n",
            "Trained for 113000 times,\n",
            "Trained for 114000 times,\n",
            "Trained for 115000 times,\n",
            "Accuarcy Test:  0.9597\n",
            "Trained for 23 epoch(s), loss = 0.11764708127096606, test = 0.9597\n",
            "Trained for 116000 times,\n",
            "Trained for 117000 times,\n",
            "Trained for 118000 times,\n",
            "Trained for 119000 times,\n",
            "Trained for 120000 times,\n",
            "Accuarcy Test:  0.9637\n",
            "Trained for 24 epoch(s), loss = 0.10527960758921363, test = 0.9637\n",
            "Trained for 121000 times,\n",
            "Trained for 122000 times,\n",
            "Trained for 123000 times,\n",
            "Trained for 124000 times,\n",
            "Trained for 125000 times,\n",
            "Accuarcy Test:  0.9621\n",
            "Trained for 25 epoch(s), loss = 0.10614953762039571, test = 0.9621\n",
            "Trained for 126000 times,\n",
            "Trained for 127000 times,\n",
            "Trained for 128000 times,\n",
            "Trained for 129000 times,\n",
            "Trained for 130000 times,\n",
            "Accuarcy Test:  0.9623\n",
            "Trained for 26 epoch(s), loss = 0.11120069522928137, test = 0.9623\n",
            "Trained for 131000 times,\n",
            "Trained for 132000 times,\n",
            "Trained for 133000 times,\n",
            "Trained for 134000 times,\n",
            "Trained for 135000 times,\n",
            "Accuarcy Test:  0.9612\n",
            "Trained for 27 epoch(s), loss = 0.10257553088741232, test = 0.9612\n",
            "Trained for 136000 times,\n",
            "Trained for 137000 times,\n",
            "Trained for 138000 times,\n",
            "Trained for 139000 times,\n",
            "Trained for 140000 times,\n",
            "Accuarcy Test:  0.9652\n",
            "Trained for 28 epoch(s), loss = 0.09608908766034069, test = 0.9652\n",
            "Trained for 141000 times,\n",
            "Trained for 142000 times,\n",
            "Trained for 143000 times,\n",
            "Trained for 144000 times,\n",
            "Trained for 145000 times,\n",
            "Accuarcy Test:  0.9648\n",
            "Trained for 29 epoch(s), loss = 0.09586412099350604, test = 0.9648\n",
            "Trained for 146000 times,\n",
            "Trained for 147000 times,\n",
            "Trained for 148000 times,\n",
            "Trained for 149000 times,\n",
            "Trained for 150000 times,\n",
            "Accuarcy Test:  0.9606\n",
            "Trained for 30 epoch(s), loss = 0.11253651504906474, test = 0.9606\n",
            "Trained for 151000 times,\n",
            "Trained for 152000 times,\n",
            "Trained for 153000 times,\n",
            "Trained for 154000 times,\n",
            "Trained for 155000 times,\n",
            "Accuarcy Test:  0.9668\n",
            "Trained for 31 epoch(s), loss = 0.09326172500249241, test = 0.9668\n",
            "Trained for 156000 times,\n",
            "Trained for 157000 times,\n",
            "Trained for 158000 times,\n",
            "Trained for 159000 times,\n",
            "Trained for 160000 times,\n",
            "Accuarcy Test:  0.9657\n",
            "Trained for 32 epoch(s), loss = 0.09205367566773627, test = 0.9657\n",
            "Trained for 161000 times,\n",
            "Trained for 162000 times,\n",
            "Trained for 163000 times,\n",
            "Trained for 164000 times,\n",
            "Trained for 165000 times,\n",
            "Accuarcy Test:  0.9679\n",
            "Trained for 33 epoch(s), loss = 0.08879362658867351, test = 0.9679\n",
            "Trained for 166000 times,\n",
            "Trained for 167000 times,\n",
            "Trained for 168000 times,\n",
            "Trained for 169000 times,\n",
            "Trained for 170000 times,\n",
            "Accuarcy Test:  0.9662\n",
            "Trained for 34 epoch(s), loss = 0.09511352079301152, test = 0.9662\n",
            "Trained for 171000 times,\n",
            "Trained for 172000 times,\n",
            "Trained for 173000 times,\n",
            "Trained for 174000 times,\n",
            "Trained for 175000 times,\n",
            "Accuarcy Test:  0.9608\n",
            "Trained for 35 epoch(s), loss = 0.1066852789727668, test = 0.9608\n",
            "Trained for 176000 times,\n",
            "Trained for 177000 times,\n",
            "Trained for 178000 times,\n",
            "Trained for 179000 times,\n",
            "Trained for 180000 times,\n",
            "Accuarcy Test:  0.9662\n",
            "Trained for 36 epoch(s), loss = 0.08888245730810203, test = 0.9662\n",
            "Trained for 181000 times,\n",
            "Trained for 182000 times,\n",
            "Trained for 183000 times,\n",
            "Trained for 184000 times,\n",
            "Trained for 185000 times,\n",
            "Accuarcy Test:  0.9675\n",
            "Trained for 37 epoch(s), loss = 0.08437755098723772, test = 0.9675\n",
            "Trained for 186000 times,\n",
            "Trained for 187000 times,\n",
            "Trained for 188000 times,\n",
            "Trained for 189000 times,\n",
            "Trained for 190000 times,\n",
            "Accuarcy Test:  0.9649\n",
            "Trained for 38 epoch(s), loss = 0.08997774917642083, test = 0.9649\n",
            "Trained for 191000 times,\n",
            "Trained for 192000 times,\n",
            "Trained for 193000 times,\n",
            "Trained for 194000 times,\n",
            "Trained for 195000 times,\n",
            "Accuarcy Test:  0.9627\n",
            "Trained for 39 epoch(s), loss = 0.09361608544080727, test = 0.9627\n",
            "Trained for 196000 times,\n",
            "Trained for 197000 times,\n",
            "Trained for 198000 times,\n",
            "Trained for 199000 times,\n",
            "Trained for 200000 times,\n",
            "Accuarcy Test:  0.9698\n",
            "Trained for 40 epoch(s), loss = 0.07508025156030781, test = 0.9698\n",
            "Training finished!\n",
            "Accuarcy Test:  0.9698\n",
            "Trained for 1000 times,\n",
            "Trained for 2000 times,\n",
            "Trained for 3000 times,\n",
            "Trained for 4000 times,\n",
            "Trained for 5000 times,\n",
            "Accuarcy Test:  0.8692\n",
            "Trained for 1 epoch(s), loss = 0.4364841921344283, test = 0.8692\n",
            "Trained for 6000 times,\n",
            "Trained for 7000 times,\n",
            "Trained for 8000 times,\n",
            "Trained for 9000 times,\n",
            "Trained for 10000 times,\n",
            "Accuarcy Test:  0.91\n",
            "Trained for 2 epoch(s), loss = 0.30848367260508885, test = 0.91\n",
            "Trained for 11000 times,\n",
            "Trained for 12000 times,\n",
            "Trained for 13000 times,\n",
            "Trained for 14000 times,\n",
            "Trained for 15000 times,\n",
            "Accuarcy Test:  0.9149\n",
            "Trained for 3 epoch(s), loss = 0.28809519521065496, test = 0.9149\n",
            "Trained for 16000 times,\n",
            "Trained for 17000 times,\n",
            "Trained for 18000 times,\n",
            "Trained for 19000 times,\n",
            "Trained for 20000 times,\n",
            "Accuarcy Test:  0.9227\n",
            "Trained for 4 epoch(s), loss = 0.2510785545229533, test = 0.9227\n",
            "Trained for 21000 times,\n",
            "Trained for 22000 times,\n",
            "Trained for 23000 times,\n",
            "Trained for 24000 times,\n",
            "Trained for 25000 times,\n",
            "Accuarcy Test:  0.9341\n",
            "Trained for 5 epoch(s), loss = 0.2200970855940078, test = 0.9341\n",
            "Trained for 26000 times,\n",
            "Trained for 27000 times,\n",
            "Trained for 28000 times,\n",
            "Trained for 29000 times,\n",
            "Trained for 30000 times,\n",
            "Accuarcy Test:  0.947\n",
            "Trained for 6 epoch(s), loss = 0.18027410565781082, test = 0.947\n",
            "Trained for 31000 times,\n",
            "Trained for 32000 times,\n",
            "Trained for 33000 times,\n",
            "Trained for 34000 times,\n",
            "Trained for 35000 times,\n",
            "Accuarcy Test:  0.9463\n",
            "Trained for 7 epoch(s), loss = 0.1802624677556789, test = 0.9463\n",
            "Trained for 36000 times,\n",
            "Trained for 37000 times,\n",
            "Trained for 38000 times,\n",
            "Trained for 39000 times,\n",
            "Trained for 40000 times,\n",
            "Accuarcy Test:  0.9435\n",
            "Trained for 8 epoch(s), loss = 0.18117072897327843, test = 0.9435\n",
            "Trained for 41000 times,\n",
            "Trained for 42000 times,\n",
            "Trained for 43000 times,\n",
            "Trained for 44000 times,\n",
            "Trained for 45000 times,\n",
            "Accuarcy Test:  0.9451\n",
            "Trained for 9 epoch(s), loss = 0.17776474398265243, test = 0.9451\n",
            "Trained for 46000 times,\n",
            "Trained for 47000 times,\n",
            "Trained for 48000 times,\n",
            "Trained for 49000 times,\n",
            "Trained for 50000 times,\n",
            "Accuarcy Test:  0.9576\n",
            "Trained for 10 epoch(s), loss = 0.13732005683686227, test = 0.9576\n",
            "Trained for 51000 times,\n",
            "Trained for 52000 times,\n",
            "Trained for 53000 times,\n",
            "Trained for 54000 times,\n",
            "Trained for 55000 times,\n",
            "Accuarcy Test:  0.9544\n",
            "Trained for 11 epoch(s), loss = 0.13968598379010905, test = 0.9544\n",
            "Trained for 56000 times,\n",
            "Trained for 57000 times,\n",
            "Trained for 58000 times,\n",
            "Trained for 59000 times,\n",
            "Trained for 60000 times,\n",
            "Accuarcy Test:  0.9506\n",
            "Trained for 12 epoch(s), loss = 0.15551782418908655, test = 0.9506\n",
            "Trained for 61000 times,\n",
            "Trained for 62000 times,\n",
            "Trained for 63000 times,\n",
            "Trained for 64000 times,\n",
            "Trained for 65000 times,\n",
            "Accuarcy Test:  0.9609\n",
            "Trained for 13 epoch(s), loss = 0.11865460393231765, test = 0.9609\n",
            "Trained for 66000 times,\n",
            "Trained for 67000 times,\n",
            "Trained for 68000 times,\n",
            "Trained for 69000 times,\n",
            "Trained for 70000 times,\n",
            "Accuarcy Test:  0.9549\n",
            "Trained for 14 epoch(s), loss = 0.1326139829465658, test = 0.9549\n",
            "Trained for 71000 times,\n",
            "Trained for 72000 times,\n",
            "Trained for 73000 times,\n",
            "Trained for 74000 times,\n",
            "Trained for 75000 times,\n",
            "Accuarcy Test:  0.9595\n",
            "Trained for 15 epoch(s), loss = 0.11956339533912032, test = 0.9595\n",
            "Trained for 76000 times,\n",
            "Trained for 77000 times,\n",
            "Trained for 78000 times,\n",
            "Trained for 79000 times,\n",
            "Trained for 80000 times,\n",
            "Accuarcy Test:  0.9571\n",
            "Trained for 16 epoch(s), loss = 0.13969536833886328, test = 0.9571\n",
            "Trained for 81000 times,\n",
            "Trained for 82000 times,\n",
            "Trained for 83000 times,\n",
            "Trained for 84000 times,\n",
            "Trained for 85000 times,\n",
            "Accuarcy Test:  0.963\n",
            "Trained for 17 epoch(s), loss = 0.10898881537546255, test = 0.963\n",
            "Trained for 86000 times,\n",
            "Trained for 87000 times,\n",
            "Trained for 88000 times,\n",
            "Trained for 89000 times,\n",
            "Trained for 90000 times,\n",
            "Accuarcy Test:  0.9626\n",
            "Trained for 18 epoch(s), loss = 0.1115863226111871, test = 0.9626\n",
            "Trained for 91000 times,\n",
            "Trained for 92000 times,\n",
            "Trained for 93000 times,\n",
            "Trained for 94000 times,\n",
            "Trained for 95000 times,\n",
            "Accuarcy Test:  0.9683\n",
            "Trained for 19 epoch(s), loss = 0.09912003996189679, test = 0.9683\n",
            "Trained for 96000 times,\n",
            "Trained for 97000 times,\n",
            "Trained for 98000 times,\n",
            "Trained for 99000 times,\n",
            "Trained for 100000 times,\n",
            "Accuarcy Test:  0.9631\n",
            "Trained for 20 epoch(s), loss = 0.10698799503460941, test = 0.9631\n",
            "Trained for 101000 times,\n",
            "Trained for 102000 times,\n",
            "Trained for 103000 times,\n",
            "Trained for 104000 times,\n",
            "Trained for 105000 times,\n",
            "Accuarcy Test:  0.9605\n",
            "Trained for 21 epoch(s), loss = 0.10797633452736147, test = 0.9605\n",
            "Trained for 106000 times,\n",
            "Trained for 107000 times,\n",
            "Trained for 108000 times,\n",
            "Trained for 109000 times,\n",
            "Trained for 110000 times,\n",
            "Accuarcy Test:  0.9668\n",
            "Trained for 22 epoch(s), loss = 0.09506641749678367, test = 0.9668\n",
            "Trained for 111000 times,\n",
            "Trained for 112000 times,\n",
            "Trained for 113000 times,\n",
            "Trained for 114000 times,\n",
            "Trained for 115000 times,\n",
            "Accuarcy Test:  0.9599\n",
            "Trained for 23 epoch(s), loss = 0.11525097457835647, test = 0.9599\n",
            "Trained for 116000 times,\n",
            "Trained for 117000 times,\n",
            "Trained for 118000 times,\n",
            "Trained for 119000 times,\n",
            "Trained for 120000 times,\n",
            "Accuarcy Test:  0.9719\n",
            "Trained for 24 epoch(s), loss = 0.08607225254745425, test = 0.9719\n",
            "Trained for 121000 times,\n",
            "Trained for 122000 times,\n",
            "Trained for 123000 times,\n",
            "Trained for 124000 times,\n",
            "Trained for 125000 times,\n",
            "Accuarcy Test:  0.9651\n",
            "Trained for 25 epoch(s), loss = 0.09725890121597998, test = 0.9651\n",
            "Trained for 126000 times,\n",
            "Trained for 127000 times,\n",
            "Trained for 128000 times,\n",
            "Trained for 129000 times,\n",
            "Trained for 130000 times,\n",
            "Accuarcy Test:  0.9714\n",
            "Trained for 26 epoch(s), loss = 0.07651620622122131, test = 0.9714\n",
            "Trained for 131000 times,\n",
            "Trained for 132000 times,\n",
            "Trained for 133000 times,\n",
            "Trained for 134000 times,\n",
            "Trained for 135000 times,\n",
            "Accuarcy Test:  0.9714\n",
            "Trained for 27 epoch(s), loss = 0.08016878348066193, test = 0.9714\n",
            "Trained for 136000 times,\n",
            "Trained for 137000 times,\n",
            "Trained for 138000 times,\n",
            "Trained for 139000 times,\n",
            "Trained for 140000 times,\n",
            "Accuarcy Test:  0.9699\n",
            "Trained for 28 epoch(s), loss = 0.07905435261371659, test = 0.9699\n",
            "Trained for 141000 times,\n",
            "Trained for 142000 times,\n",
            "Trained for 143000 times,\n",
            "Trained for 144000 times,\n",
            "Trained for 145000 times,\n",
            "Accuarcy Test:  0.9674\n",
            "Trained for 29 epoch(s), loss = 0.08892754233617257, test = 0.9674\n",
            "Trained for 146000 times,\n",
            "Trained for 147000 times,\n",
            "Trained for 148000 times,\n",
            "Trained for 149000 times,\n",
            "Trained for 150000 times,\n",
            "Accuarcy Test:  0.9696\n",
            "Trained for 30 epoch(s), loss = 0.07752466220735076, test = 0.9696\n",
            "Trained for 151000 times,\n",
            "Trained for 152000 times,\n",
            "Trained for 153000 times,\n",
            "Trained for 154000 times,\n",
            "Trained for 155000 times,\n",
            "Accuarcy Test:  0.9677\n",
            "Trained for 31 epoch(s), loss = 0.08468015453357071, test = 0.9677\n",
            "Trained for 156000 times,\n",
            "Trained for 157000 times,\n",
            "Trained for 158000 times,\n",
            "Trained for 159000 times,\n",
            "Trained for 160000 times,\n",
            "Accuarcy Test:  0.9702\n",
            "Trained for 32 epoch(s), loss = 0.07164857392880215, test = 0.9702\n",
            "Trained for 161000 times,\n",
            "Trained for 162000 times,\n",
            "Trained for 163000 times,\n",
            "Trained for 164000 times,\n",
            "Trained for 165000 times,\n",
            "Accuarcy Test:  0.9675\n",
            "Trained for 33 epoch(s), loss = 0.08379962730246446, test = 0.9675\n",
            "Trained for 166000 times,\n",
            "Trained for 167000 times,\n",
            "Trained for 168000 times,\n",
            "Trained for 169000 times,\n",
            "Trained for 170000 times,\n",
            "Accuarcy Test:  0.9628\n",
            "Trained for 34 epoch(s), loss = 0.09528596456237645, test = 0.9628\n",
            "Trained for 171000 times,\n",
            "Trained for 172000 times,\n",
            "Trained for 173000 times,\n",
            "Trained for 174000 times,\n",
            "Trained for 175000 times,\n",
            "Accuarcy Test:  0.9693\n",
            "Trained for 35 epoch(s), loss = 0.07736681919062735, test = 0.9693\n",
            "Trained for 176000 times,\n",
            "Trained for 177000 times,\n",
            "Trained for 178000 times,\n",
            "Trained for 179000 times,\n",
            "Trained for 180000 times,\n",
            "Accuarcy Test:  0.9739\n",
            "Trained for 36 epoch(s), loss = 0.06657595924898273, test = 0.9739\n",
            "Trained for 181000 times,\n",
            "Trained for 182000 times,\n",
            "Trained for 183000 times,\n",
            "Trained for 184000 times,\n",
            "Trained for 185000 times,\n",
            "Accuarcy Test:  0.9724\n",
            "Trained for 37 epoch(s), loss = 0.07168413872637919, test = 0.9724\n",
            "Trained for 186000 times,\n",
            "Trained for 187000 times,\n",
            "Trained for 188000 times,\n",
            "Trained for 189000 times,\n",
            "Trained for 190000 times,\n",
            "Accuarcy Test:  0.9729\n",
            "Trained for 38 epoch(s), loss = 0.0678969278075635, test = 0.9729\n",
            "Trained for 191000 times,\n",
            "Trained for 192000 times,\n",
            "Trained for 193000 times,\n",
            "Trained for 194000 times,\n",
            "Trained for 195000 times,\n",
            "Accuarcy Test:  0.9709\n",
            "Trained for 39 epoch(s), loss = 0.06796170440986281, test = 0.9709\n",
            "Trained for 196000 times,\n",
            "Trained for 197000 times,\n",
            "Trained for 198000 times,\n",
            "Trained for 199000 times,\n",
            "Trained for 200000 times,\n",
            "Accuarcy Test:  0.9721\n",
            "Trained for 40 epoch(s), loss = 0.06851121677823281, test = 0.9721\n",
            "Training finished!\n",
            "Accuarcy Test:  0.9721\n",
            "Trained for 1000 times,\n",
            "Trained for 2000 times,\n",
            "Trained for 3000 times,\n",
            "Trained for 4000 times,\n",
            "Trained for 5000 times,\n",
            "Accuarcy Test:  0.9009\n",
            "Trained for 1 epoch(s), loss = 0.3574583803544754, test = 0.9009\n",
            "Trained for 6000 times,\n",
            "Trained for 7000 times,\n",
            "Trained for 8000 times,\n",
            "Trained for 9000 times,\n",
            "Trained for 10000 times,\n",
            "Accuarcy Test:  0.9042\n",
            "Trained for 2 epoch(s), loss = 0.31264257035923904, test = 0.9042\n",
            "Trained for 11000 times,\n",
            "Trained for 12000 times,\n",
            "Trained for 13000 times,\n",
            "Trained for 14000 times,\n",
            "Trained for 15000 times,\n",
            "Accuarcy Test:  0.9261\n",
            "Trained for 3 epoch(s), loss = 0.2502921035122968, test = 0.9261\n",
            "Trained for 16000 times,\n",
            "Trained for 17000 times,\n",
            "Trained for 18000 times,\n",
            "Trained for 19000 times,\n",
            "Trained for 20000 times,\n",
            "Accuarcy Test:  0.9303\n",
            "Trained for 4 epoch(s), loss = 0.23470848564794008, test = 0.9303\n",
            "Trained for 21000 times,\n",
            "Trained for 22000 times,\n",
            "Trained for 23000 times,\n",
            "Trained for 24000 times,\n",
            "Trained for 25000 times,\n",
            "Accuarcy Test:  0.9423\n",
            "Trained for 5 epoch(s), loss = 0.19292965663816808, test = 0.9423\n",
            "Trained for 26000 times,\n",
            "Trained for 27000 times,\n",
            "Trained for 28000 times,\n",
            "Trained for 29000 times,\n",
            "Trained for 30000 times,\n",
            "Accuarcy Test:  0.9409\n",
            "Trained for 6 epoch(s), loss = 0.18749955362393572, test = 0.9409\n",
            "Trained for 31000 times,\n",
            "Trained for 32000 times,\n",
            "Trained for 33000 times,\n",
            "Trained for 34000 times,\n",
            "Trained for 35000 times,\n",
            "Accuarcy Test:  0.9492\n",
            "Trained for 7 epoch(s), loss = 0.16399438793838617, test = 0.9492\n",
            "Trained for 36000 times,\n",
            "Trained for 37000 times,\n",
            "Trained for 38000 times,\n",
            "Trained for 39000 times,\n",
            "Trained for 40000 times,\n",
            "Accuarcy Test:  0.954\n",
            "Trained for 8 epoch(s), loss = 0.1478542079490384, test = 0.954\n",
            "Trained for 41000 times,\n",
            "Trained for 42000 times,\n",
            "Trained for 43000 times,\n",
            "Trained for 44000 times,\n",
            "Trained for 45000 times,\n",
            "Accuarcy Test:  0.951\n",
            "Trained for 9 epoch(s), loss = 0.1476687758327363, test = 0.951\n",
            "Trained for 46000 times,\n",
            "Trained for 47000 times,\n",
            "Trained for 48000 times,\n",
            "Trained for 49000 times,\n",
            "Trained for 50000 times,\n",
            "Accuarcy Test:  0.9584\n",
            "Trained for 10 epoch(s), loss = 0.1322634920878495, test = 0.9584\n",
            "Trained for 51000 times,\n",
            "Trained for 52000 times,\n",
            "Trained for 53000 times,\n",
            "Trained for 54000 times,\n",
            "Trained for 55000 times,\n",
            "Accuarcy Test:  0.9574\n",
            "Trained for 11 epoch(s), loss = 0.13247576832978966, test = 0.9574\n",
            "Trained for 56000 times,\n",
            "Trained for 57000 times,\n",
            "Trained for 58000 times,\n",
            "Trained for 59000 times,\n",
            "Trained for 60000 times,\n",
            "Accuarcy Test:  0.9601\n",
            "Trained for 12 epoch(s), loss = 0.12726331486040884, test = 0.9601\n",
            "Trained for 61000 times,\n",
            "Trained for 62000 times,\n",
            "Trained for 63000 times,\n",
            "Trained for 64000 times,\n",
            "Trained for 65000 times,\n",
            "Accuarcy Test:  0.9582\n",
            "Trained for 13 epoch(s), loss = 0.12924073660321422, test = 0.9582\n",
            "Trained for 66000 times,\n",
            "Trained for 67000 times,\n",
            "Trained for 68000 times,\n",
            "Trained for 69000 times,\n",
            "Trained for 70000 times,\n",
            "Accuarcy Test:  0.9663\n",
            "Trained for 14 epoch(s), loss = 0.1073708335260915, test = 0.9663\n",
            "Trained for 71000 times,\n",
            "Trained for 72000 times,\n",
            "Trained for 73000 times,\n",
            "Trained for 74000 times,\n",
            "Trained for 75000 times,\n",
            "Accuarcy Test:  0.9591\n",
            "Trained for 15 epoch(s), loss = 0.12213154539499475, test = 0.9591\n",
            "Trained for 76000 times,\n",
            "Trained for 77000 times,\n",
            "Trained for 78000 times,\n",
            "Trained for 79000 times,\n",
            "Trained for 80000 times,\n",
            "Accuarcy Test:  0.9655\n",
            "Trained for 16 epoch(s), loss = 0.11130455415919037, test = 0.9655\n",
            "Trained for 81000 times,\n",
            "Trained for 82000 times,\n",
            "Trained for 83000 times,\n",
            "Trained for 84000 times,\n",
            "Trained for 85000 times,\n",
            "Accuarcy Test:  0.9659\n",
            "Trained for 17 epoch(s), loss = 0.1071000239756854, test = 0.9659\n",
            "Trained for 86000 times,\n",
            "Trained for 87000 times,\n",
            "Trained for 88000 times,\n",
            "Trained for 89000 times,\n",
            "Trained for 90000 times,\n",
            "Accuarcy Test:  0.9676\n",
            "Trained for 18 epoch(s), loss = 0.09190318742628781, test = 0.9676\n",
            "Trained for 91000 times,\n",
            "Trained for 92000 times,\n",
            "Trained for 93000 times,\n",
            "Trained for 94000 times,\n",
            "Trained for 95000 times,\n",
            "Accuarcy Test:  0.966\n",
            "Trained for 19 epoch(s), loss = 0.09872984404232268, test = 0.966\n",
            "Trained for 96000 times,\n",
            "Trained for 97000 times,\n",
            "Trained for 98000 times,\n",
            "Trained for 99000 times,\n",
            "Trained for 100000 times,\n",
            "Accuarcy Test:  0.9662\n",
            "Trained for 20 epoch(s), loss = 0.09306196806083485, test = 0.9662\n",
            "Trained for 101000 times,\n",
            "Trained for 102000 times,\n",
            "Trained for 103000 times,\n",
            "Trained for 104000 times,\n",
            "Trained for 105000 times,\n",
            "Accuarcy Test:  0.9702\n",
            "Trained for 21 epoch(s), loss = 0.08218843096273012, test = 0.9702\n",
            "Trained for 106000 times,\n",
            "Trained for 107000 times,\n",
            "Trained for 108000 times,\n",
            "Trained for 109000 times,\n",
            "Trained for 110000 times,\n",
            "Accuarcy Test:  0.9592\n",
            "Trained for 22 epoch(s), loss = 0.11258632631231814, test = 0.9592\n",
            "Trained for 111000 times,\n",
            "Trained for 112000 times,\n",
            "Trained for 113000 times,\n",
            "Trained for 114000 times,\n",
            "Trained for 115000 times,\n",
            "Accuarcy Test:  0.9678\n",
            "Trained for 23 epoch(s), loss = 0.09176834525886557, test = 0.9678\n",
            "Trained for 116000 times,\n",
            "Trained for 117000 times,\n",
            "Trained for 118000 times,\n",
            "Trained for 119000 times,\n",
            "Trained for 120000 times,\n",
            "Accuarcy Test:  0.9686\n",
            "Trained for 24 epoch(s), loss = 0.09398800275565578, test = 0.9686\n",
            "Trained for 121000 times,\n",
            "Trained for 122000 times,\n",
            "Trained for 123000 times,\n",
            "Trained for 124000 times,\n",
            "Trained for 125000 times,\n",
            "Accuarcy Test:  0.9699\n",
            "Trained for 25 epoch(s), loss = 0.0804833034120523, test = 0.9699\n",
            "Trained for 126000 times,\n",
            "Trained for 127000 times,\n",
            "Trained for 128000 times,\n",
            "Trained for 129000 times,\n",
            "Trained for 130000 times,\n",
            "Accuarcy Test:  0.9662\n",
            "Trained for 26 epoch(s), loss = 0.08949858998043099, test = 0.9662\n",
            "Trained for 131000 times,\n",
            "Trained for 132000 times,\n",
            "Trained for 133000 times,\n",
            "Trained for 134000 times,\n",
            "Trained for 135000 times,\n",
            "Accuarcy Test:  0.9667\n",
            "Trained for 27 epoch(s), loss = 0.08455597019072944, test = 0.9667\n",
            "Trained for 136000 times,\n",
            "Trained for 137000 times,\n",
            "Trained for 138000 times,\n",
            "Trained for 139000 times,\n",
            "Trained for 140000 times,\n",
            "Accuarcy Test:  0.9699\n",
            "Trained for 28 epoch(s), loss = 0.08072721752471229, test = 0.9699\n",
            "Trained for 141000 times,\n",
            "Trained for 142000 times,\n",
            "Trained for 143000 times,\n",
            "Trained for 144000 times,\n",
            "Trained for 145000 times,\n",
            "Accuarcy Test:  0.9679\n",
            "Trained for 29 epoch(s), loss = 0.08261128615151357, test = 0.9679\n",
            "Trained for 146000 times,\n",
            "Trained for 147000 times,\n",
            "Trained for 148000 times,\n",
            "Trained for 149000 times,\n",
            "Trained for 150000 times,\n",
            "Accuarcy Test:  0.9728\n",
            "Trained for 30 epoch(s), loss = 0.06823307915222505, test = 0.9728\n",
            "Trained for 151000 times,\n",
            "Trained for 152000 times,\n",
            "Trained for 153000 times,\n",
            "Trained for 154000 times,\n",
            "Trained for 155000 times,\n",
            "Accuarcy Test:  0.9727\n",
            "Trained for 31 epoch(s), loss = 0.07202003119771787, test = 0.9727\n",
            "Trained for 156000 times,\n",
            "Trained for 157000 times,\n",
            "Trained for 158000 times,\n",
            "Trained for 159000 times,\n",
            "Trained for 160000 times,\n",
            "Accuarcy Test:  0.9722\n",
            "Trained for 32 epoch(s), loss = 0.0631490410179854, test = 0.9722\n",
            "Trained for 161000 times,\n",
            "Trained for 162000 times,\n",
            "Trained for 163000 times,\n",
            "Trained for 164000 times,\n",
            "Trained for 165000 times,\n",
            "Accuarcy Test:  0.971\n",
            "Trained for 33 epoch(s), loss = 0.07360710811746347, test = 0.971\n",
            "Trained for 166000 times,\n",
            "Trained for 167000 times,\n",
            "Trained for 168000 times,\n",
            "Trained for 169000 times,\n",
            "Trained for 170000 times,\n",
            "Accuarcy Test:  0.9739\n",
            "Trained for 34 epoch(s), loss = 0.0648210535313299, test = 0.9739\n",
            "Trained for 171000 times,\n",
            "Trained for 172000 times,\n",
            "Trained for 173000 times,\n",
            "Trained for 174000 times,\n",
            "Trained for 175000 times,\n",
            "Accuarcy Test:  0.9747\n",
            "Trained for 35 epoch(s), loss = 0.06475753458094877, test = 0.9747\n",
            "Trained for 176000 times,\n",
            "Trained for 177000 times,\n",
            "Trained for 178000 times,\n",
            "Trained for 179000 times,\n",
            "Trained for 180000 times,\n",
            "Accuarcy Test:  0.9733\n",
            "Trained for 36 epoch(s), loss = 0.06858170899236801, test = 0.9733\n",
            "Trained for 181000 times,\n",
            "Trained for 182000 times,\n",
            "Trained for 183000 times,\n",
            "Trained for 184000 times,\n",
            "Trained for 185000 times,\n",
            "Accuarcy Test:  0.963\n",
            "Trained for 37 epoch(s), loss = 0.0844001502299188, test = 0.963\n",
            "Trained for 186000 times,\n",
            "Trained for 187000 times,\n",
            "Trained for 188000 times,\n",
            "Trained for 189000 times,\n",
            "Trained for 190000 times,\n",
            "Accuarcy Test:  0.9675\n",
            "Trained for 38 epoch(s), loss = 0.07408711229424235, test = 0.9675\n",
            "Trained for 191000 times,\n",
            "Trained for 192000 times,\n",
            "Trained for 193000 times,\n",
            "Trained for 194000 times,\n",
            "Trained for 195000 times,\n",
            "Accuarcy Test:  0.9705\n",
            "Trained for 39 epoch(s), loss = 0.07308981118270755, test = 0.9705\n",
            "Trained for 196000 times,\n",
            "Trained for 197000 times,\n",
            "Trained for 198000 times,\n",
            "Trained for 199000 times,\n",
            "Trained for 200000 times,\n",
            "Accuarcy Test:  0.9747\n",
            "Trained for 40 epoch(s), loss = 0.05963993757891511, test = 0.9747\n",
            "Training finished!\n",
            "Accuarcy Test:  0.9747\n",
            "Trained for 1000 times,\n",
            "Trained for 2000 times,\n",
            "Trained for 3000 times,\n",
            "Trained for 4000 times,\n",
            "Trained for 5000 times,\n",
            "Accuarcy Test:  0.8808\n",
            "Trained for 1 epoch(s), loss = 0.3929397386946356, test = 0.8808\n",
            "Trained for 6000 times,\n",
            "Trained for 7000 times,\n",
            "Trained for 8000 times,\n",
            "Trained for 9000 times,\n",
            "Trained for 10000 times,\n",
            "Accuarcy Test:  0.9118\n",
            "Trained for 2 epoch(s), loss = 0.28942193294075613, test = 0.9118\n",
            "Trained for 11000 times,\n",
            "Trained for 12000 times,\n",
            "Trained for 13000 times,\n",
            "Trained for 14000 times,\n",
            "Trained for 15000 times,\n",
            "Accuarcy Test:  0.9205\n",
            "Trained for 3 epoch(s), loss = 0.26541339204186426, test = 0.9205\n",
            "Trained for 16000 times,\n",
            "Trained for 17000 times,\n",
            "Trained for 18000 times,\n",
            "Trained for 19000 times,\n",
            "Trained for 20000 times,\n",
            "Accuarcy Test:  0.9409\n",
            "Trained for 4 epoch(s), loss = 0.20547184560947013, test = 0.9409\n",
            "Trained for 21000 times,\n",
            "Trained for 22000 times,\n",
            "Trained for 23000 times,\n",
            "Trained for 24000 times,\n",
            "Trained for 25000 times,\n",
            "Accuarcy Test:  0.9476\n",
            "Trained for 5 epoch(s), loss = 0.18308999011171037, test = 0.9476\n",
            "Trained for 26000 times,\n",
            "Trained for 27000 times,\n",
            "Trained for 28000 times,\n",
            "Trained for 29000 times,\n",
            "Trained for 30000 times,\n",
            "Accuarcy Test:  0.9498\n",
            "Trained for 6 epoch(s), loss = 0.17069090897355654, test = 0.9498\n",
            "Trained for 31000 times,\n",
            "Trained for 32000 times,\n",
            "Trained for 33000 times,\n",
            "Trained for 34000 times,\n",
            "Trained for 35000 times,\n",
            "Accuarcy Test:  0.935\n",
            "Trained for 7 epoch(s), loss = 0.19155434942659433, test = 0.935\n",
            "Trained for 36000 times,\n",
            "Trained for 37000 times,\n",
            "Trained for 38000 times,\n",
            "Trained for 39000 times,\n",
            "Trained for 40000 times,\n",
            "Accuarcy Test:  0.948\n",
            "Trained for 8 epoch(s), loss = 0.16705996851816782, test = 0.948\n",
            "Trained for 41000 times,\n",
            "Trained for 42000 times,\n",
            "Trained for 43000 times,\n",
            "Trained for 44000 times,\n",
            "Trained for 45000 times,\n",
            "Accuarcy Test:  0.9565\n",
            "Trained for 9 epoch(s), loss = 0.1355775400369673, test = 0.9565\n",
            "Trained for 46000 times,\n",
            "Trained for 47000 times,\n",
            "Trained for 48000 times,\n",
            "Trained for 49000 times,\n",
            "Trained for 50000 times,\n",
            "Accuarcy Test:  0.9609\n",
            "Trained for 10 epoch(s), loss = 0.12601619639416145, test = 0.9609\n",
            "Trained for 51000 times,\n",
            "Trained for 52000 times,\n",
            "Trained for 53000 times,\n",
            "Trained for 54000 times,\n",
            "Trained for 55000 times,\n",
            "Accuarcy Test:  0.9631\n",
            "Trained for 11 epoch(s), loss = 0.12015233343636546, test = 0.9631\n",
            "Trained for 56000 times,\n",
            "Trained for 57000 times,\n",
            "Trained for 58000 times,\n",
            "Trained for 59000 times,\n",
            "Trained for 60000 times,\n",
            "Accuarcy Test:  0.9615\n",
            "Trained for 12 epoch(s), loss = 0.11488366739740162, test = 0.9615\n",
            "Trained for 61000 times,\n",
            "Trained for 62000 times,\n",
            "Trained for 63000 times,\n",
            "Trained for 64000 times,\n",
            "Trained for 65000 times,\n",
            "Accuarcy Test:  0.9609\n",
            "Trained for 13 epoch(s), loss = 0.11010977986661816, test = 0.9609\n",
            "Trained for 66000 times,\n",
            "Trained for 67000 times,\n",
            "Trained for 68000 times,\n",
            "Trained for 69000 times,\n",
            "Trained for 70000 times,\n",
            "Accuarcy Test:  0.9626\n",
            "Trained for 14 epoch(s), loss = 0.11380793091955926, test = 0.9626\n",
            "Trained for 71000 times,\n",
            "Trained for 72000 times,\n",
            "Trained for 73000 times,\n",
            "Trained for 74000 times,\n",
            "Trained for 75000 times,\n",
            "Accuarcy Test:  0.961\n",
            "Trained for 15 epoch(s), loss = 0.10996915716295828, test = 0.961\n",
            "Trained for 76000 times,\n",
            "Trained for 77000 times,\n",
            "Trained for 78000 times,\n",
            "Trained for 79000 times,\n",
            "Trained for 80000 times,\n",
            "Accuarcy Test:  0.9678\n",
            "Trained for 16 epoch(s), loss = 0.09778002755462901, test = 0.9678\n",
            "Trained for 81000 times,\n",
            "Trained for 82000 times,\n",
            "Trained for 83000 times,\n",
            "Trained for 84000 times,\n",
            "Trained for 85000 times,\n",
            "Accuarcy Test:  0.9652\n",
            "Trained for 17 epoch(s), loss = 0.09476872456977552, test = 0.9652\n",
            "Trained for 86000 times,\n",
            "Trained for 87000 times,\n",
            "Trained for 88000 times,\n",
            "Trained for 89000 times,\n",
            "Trained for 90000 times,\n",
            "Accuarcy Test:  0.959\n",
            "Trained for 18 epoch(s), loss = 0.1070422553038748, test = 0.959\n",
            "Trained for 91000 times,\n",
            "Trained for 92000 times,\n",
            "Trained for 93000 times,\n",
            "Trained for 94000 times,\n",
            "Trained for 95000 times,\n",
            "Accuarcy Test:  0.9678\n",
            "Trained for 19 epoch(s), loss = 0.08764623933340804, test = 0.9678\n",
            "Trained for 96000 times,\n",
            "Trained for 97000 times,\n",
            "Trained for 98000 times,\n",
            "Trained for 99000 times,\n",
            "Trained for 100000 times,\n",
            "Accuarcy Test:  0.9602\n",
            "Trained for 20 epoch(s), loss = 0.10743507958212825, test = 0.9602\n",
            "Trained for 101000 times,\n",
            "Trained for 102000 times,\n",
            "Trained for 103000 times,\n",
            "Trained for 104000 times,\n",
            "Trained for 105000 times,\n",
            "Accuarcy Test:  0.9678\n",
            "Trained for 21 epoch(s), loss = 0.08980230188243356, test = 0.9678\n",
            "Trained for 106000 times,\n",
            "Trained for 107000 times,\n",
            "Trained for 108000 times,\n",
            "Trained for 109000 times,\n",
            "Trained for 110000 times,\n",
            "Accuarcy Test:  0.9715\n",
            "Trained for 22 epoch(s), loss = 0.08480172638048508, test = 0.9715\n",
            "Trained for 111000 times,\n",
            "Trained for 112000 times,\n",
            "Trained for 113000 times,\n",
            "Trained for 114000 times,\n",
            "Trained for 115000 times,\n",
            "Accuarcy Test:  0.9702\n",
            "Trained for 23 epoch(s), loss = 0.08164703253056022, test = 0.9702\n",
            "Trained for 116000 times,\n",
            "Trained for 117000 times,\n",
            "Trained for 118000 times,\n",
            "Trained for 119000 times,\n",
            "Trained for 120000 times,\n",
            "Accuarcy Test:  0.9731\n",
            "Trained for 24 epoch(s), loss = 0.07211453288130315, test = 0.9731\n",
            "Trained for 121000 times,\n",
            "Trained for 122000 times,\n",
            "Trained for 123000 times,\n",
            "Trained for 124000 times,\n",
            "Trained for 125000 times,\n",
            "Accuarcy Test:  0.9729\n",
            "Trained for 25 epoch(s), loss = 0.06936617245071228, test = 0.9729\n",
            "Trained for 126000 times,\n",
            "Trained for 127000 times,\n",
            "Trained for 128000 times,\n",
            "Trained for 129000 times,\n",
            "Trained for 130000 times,\n",
            "Accuarcy Test:  0.9748\n",
            "Trained for 26 epoch(s), loss = 0.06484121430987091, test = 0.9748\n",
            "Trained for 131000 times,\n",
            "Trained for 132000 times,\n",
            "Trained for 133000 times,\n",
            "Trained for 134000 times,\n",
            "Trained for 135000 times,\n",
            "Accuarcy Test:  0.9721\n",
            "Trained for 27 epoch(s), loss = 0.0721427428131314, test = 0.9721\n",
            "Trained for 136000 times,\n",
            "Trained for 137000 times,\n",
            "Trained for 138000 times,\n",
            "Trained for 139000 times,\n",
            "Trained for 140000 times,\n",
            "Accuarcy Test:  0.9722\n",
            "Trained for 28 epoch(s), loss = 0.07080426939610776, test = 0.9722\n",
            "Trained for 141000 times,\n",
            "Trained for 142000 times,\n",
            "Trained for 143000 times,\n",
            "Trained for 144000 times,\n",
            "Trained for 145000 times,\n",
            "Accuarcy Test:  0.9639\n",
            "Trained for 29 epoch(s), loss = 0.08894433379277956, test = 0.9639\n",
            "Trained for 146000 times,\n",
            "Trained for 147000 times,\n",
            "Trained for 148000 times,\n",
            "Trained for 149000 times,\n",
            "Trained for 150000 times,\n",
            "Accuarcy Test:  0.9761\n",
            "Trained for 30 epoch(s), loss = 0.06400320607007769, test = 0.9761\n",
            "Trained for 151000 times,\n",
            "Trained for 152000 times,\n",
            "Trained for 153000 times,\n",
            "Trained for 154000 times,\n",
            "Trained for 155000 times,\n",
            "Accuarcy Test:  0.9725\n",
            "Trained for 31 epoch(s), loss = 0.06370469805518231, test = 0.9725\n",
            "Trained for 156000 times,\n",
            "Trained for 157000 times,\n",
            "Trained for 158000 times,\n",
            "Trained for 159000 times,\n",
            "Trained for 160000 times,\n",
            "Accuarcy Test:  0.9731\n",
            "Trained for 32 epoch(s), loss = 0.06106725848270614, test = 0.9731\n",
            "Trained for 161000 times,\n",
            "Trained for 162000 times,\n",
            "Trained for 163000 times,\n",
            "Trained for 164000 times,\n",
            "Trained for 165000 times,\n",
            "Accuarcy Test:  0.9761\n",
            "Trained for 33 epoch(s), loss = 0.059686309693121144, test = 0.9761\n",
            "Trained for 166000 times,\n",
            "Trained for 167000 times,\n",
            "Trained for 168000 times,\n",
            "Trained for 169000 times,\n",
            "Trained for 170000 times,\n",
            "Accuarcy Test:  0.9771\n",
            "Trained for 34 epoch(s), loss = 0.05541350987943737, test = 0.9771\n",
            "Trained for 171000 times,\n",
            "Trained for 172000 times,\n",
            "Trained for 173000 times,\n",
            "Trained for 174000 times,\n",
            "Trained for 175000 times,\n",
            "Accuarcy Test:  0.9767\n",
            "Trained for 35 epoch(s), loss = 0.055744502413366356, test = 0.9767\n",
            "Trained for 176000 times,\n",
            "Trained for 177000 times,\n",
            "Trained for 178000 times,\n",
            "Trained for 179000 times,\n",
            "Trained for 180000 times,\n",
            "Accuarcy Test:  0.9754\n",
            "Trained for 36 epoch(s), loss = 0.0581993153727381, test = 0.9754\n",
            "Trained for 181000 times,\n",
            "Trained for 182000 times,\n",
            "Trained for 183000 times,\n",
            "Trained for 184000 times,\n",
            "Trained for 185000 times,\n",
            "Accuarcy Test:  0.9748\n",
            "Trained for 37 epoch(s), loss = 0.05825684030808853, test = 0.9748\n",
            "Trained for 186000 times,\n",
            "Trained for 187000 times,\n",
            "Trained for 188000 times,\n",
            "Trained for 189000 times,\n",
            "Trained for 190000 times,\n",
            "Accuarcy Test:  0.9758\n",
            "Trained for 38 epoch(s), loss = 0.05571643957958934, test = 0.9758\n",
            "Trained for 191000 times,\n",
            "Trained for 192000 times,\n",
            "Trained for 193000 times,\n",
            "Trained for 194000 times,\n",
            "Trained for 195000 times,\n",
            "Accuarcy Test:  0.9719\n",
            "Trained for 39 epoch(s), loss = 0.06044114538434081, test = 0.9719\n",
            "Trained for 196000 times,\n",
            "Trained for 197000 times,\n",
            "Trained for 198000 times,\n",
            "Trained for 199000 times,\n",
            "Trained for 200000 times,\n",
            "Accuarcy Test:  0.9747\n",
            "Trained for 40 epoch(s), loss = 0.05686599611312375, test = 0.9747\n",
            "Training finished!\n",
            "Accuarcy Test:  0.9747\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chạy code trên trường hợp đã chọn"
      ],
      "metadata": {
        "id": "WmYBHvNC_MGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_iterations = 200000\n",
        "learning_rate = 0.01\n",
        "num_inputs = 28*28\n",
        "num_outputs = 10\n",
        "hidden_size = 256\n",
        "acc_per_hidden_size = []\n",
        "# data fitting, training and accuracy evaluation\n",
        "model = NN(num_inputs, hidden_size, num_outputs)\n",
        "cost_dict, tests_dict = model.train(x_train, y_train, num_iterations=num_iterations, learning_rate=learning_rate)\n",
        "tmp, _ = model.testing(x_test, y_test)\n",
        "acc_per_hidden_size.append(tmp)"
      ],
      "metadata": {
        "id": "TfZ3SgAn7yl-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ac3d79e-0502-4a67-d0a2-c8d745b43cc2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trained for 1000 times,\n",
            "Trained for 2000 times,\n",
            "Trained for 3000 times,\n",
            "Trained for 4000 times,\n",
            "Trained for 5000 times,\n",
            "Accuarcy Test:  0.8802\n",
            "Trained for 1 epoch(s), loss = 0.40287857605621, test = 0.8802\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:138: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trained for 6000 times,\n",
            "Trained for 7000 times,\n",
            "Trained for 8000 times,\n",
            "Trained for 9000 times,\n",
            "Trained for 10000 times,\n",
            "Accuarcy Test:  0.916\n",
            "Trained for 2 epoch(s), loss = 0.29169913906800116, test = 0.916\n",
            "Trained for 11000 times,\n",
            "Trained for 12000 times,\n",
            "Trained for 13000 times,\n",
            "Trained for 14000 times,\n",
            "Trained for 15000 times,\n",
            "Accuarcy Test:  0.9277\n",
            "Trained for 3 epoch(s), loss = 0.2516264278883902, test = 0.9277\n",
            "Trained for 16000 times,\n",
            "Trained for 17000 times,\n",
            "Trained for 18000 times,\n",
            "Trained for 19000 times,\n",
            "Trained for 20000 times,\n",
            "Accuarcy Test:  0.9403\n",
            "Trained for 4 epoch(s), loss = 0.20511688952648566, test = 0.9403\n",
            "Trained for 21000 times,\n",
            "Trained for 22000 times,\n",
            "Trained for 23000 times,\n",
            "Trained for 24000 times,\n",
            "Trained for 25000 times,\n",
            "Accuarcy Test:  0.9371\n",
            "Trained for 5 epoch(s), loss = 0.20410086513850412, test = 0.9371\n",
            "Trained for 26000 times,\n",
            "Trained for 27000 times,\n",
            "Trained for 28000 times,\n",
            "Trained for 29000 times,\n",
            "Trained for 30000 times,\n",
            "Accuarcy Test:  0.9493\n",
            "Trained for 6 epoch(s), loss = 0.17220196037266558, test = 0.9493\n",
            "Trained for 31000 times,\n",
            "Trained for 32000 times,\n",
            "Trained for 33000 times,\n",
            "Trained for 34000 times,\n",
            "Trained for 35000 times,\n",
            "Accuarcy Test:  0.9512\n",
            "Trained for 7 epoch(s), loss = 0.17004558071266654, test = 0.9512\n",
            "Trained for 36000 times,\n",
            "Trained for 37000 times,\n",
            "Trained for 38000 times,\n",
            "Trained for 39000 times,\n",
            "Trained for 40000 times,\n",
            "Accuarcy Test:  0.9523\n",
            "Trained for 8 epoch(s), loss = 0.152242712301499, test = 0.9523\n",
            "Trained for 41000 times,\n",
            "Trained for 42000 times,\n",
            "Trained for 43000 times,\n",
            "Trained for 44000 times,\n",
            "Trained for 45000 times,\n",
            "Accuarcy Test:  0.956\n",
            "Trained for 9 epoch(s), loss = 0.13871097188130718, test = 0.956\n",
            "Trained for 46000 times,\n",
            "Trained for 47000 times,\n",
            "Trained for 48000 times,\n",
            "Trained for 49000 times,\n",
            "Trained for 50000 times,\n",
            "Accuarcy Test:  0.9575\n",
            "Trained for 10 epoch(s), loss = 0.13754967574496388, test = 0.9575\n",
            "Trained for 51000 times,\n",
            "Trained for 52000 times,\n",
            "Trained for 53000 times,\n",
            "Trained for 54000 times,\n",
            "Trained for 55000 times,\n",
            "Accuarcy Test:  0.9589\n",
            "Trained for 11 epoch(s), loss = 0.12635944279589942, test = 0.9589\n",
            "Trained for 56000 times,\n",
            "Trained for 57000 times,\n",
            "Trained for 58000 times,\n",
            "Trained for 59000 times,\n",
            "Trained for 60000 times,\n",
            "Accuarcy Test:  0.9576\n",
            "Trained for 12 epoch(s), loss = 0.13070876708135076, test = 0.9576\n",
            "Trained for 61000 times,\n",
            "Trained for 62000 times,\n",
            "Trained for 63000 times,\n",
            "Trained for 64000 times,\n",
            "Trained for 65000 times,\n",
            "Accuarcy Test:  0.9606\n",
            "Trained for 13 epoch(s), loss = 0.12539614703205298, test = 0.9606\n",
            "Trained for 66000 times,\n",
            "Trained for 67000 times,\n",
            "Trained for 68000 times,\n",
            "Trained for 69000 times,\n",
            "Trained for 70000 times,\n",
            "Accuarcy Test:  0.9612\n",
            "Trained for 14 epoch(s), loss = 0.11405545579067666, test = 0.9612\n",
            "Trained for 71000 times,\n",
            "Trained for 72000 times,\n",
            "Trained for 73000 times,\n",
            "Trained for 74000 times,\n",
            "Trained for 75000 times,\n",
            "Accuarcy Test:  0.9606\n",
            "Trained for 15 epoch(s), loss = 0.11802749623222664, test = 0.9606\n",
            "Trained for 76000 times,\n",
            "Trained for 77000 times,\n",
            "Trained for 78000 times,\n",
            "Trained for 79000 times,\n",
            "Trained for 80000 times,\n",
            "Accuarcy Test:  0.9664\n",
            "Trained for 16 epoch(s), loss = 0.1029386894552586, test = 0.9664\n",
            "Trained for 81000 times,\n",
            "Trained for 82000 times,\n",
            "Trained for 83000 times,\n",
            "Trained for 84000 times,\n",
            "Trained for 85000 times,\n",
            "Accuarcy Test:  0.9634\n",
            "Trained for 17 epoch(s), loss = 0.10338428587223569, test = 0.9634\n",
            "Trained for 86000 times,\n",
            "Trained for 87000 times,\n",
            "Trained for 88000 times,\n",
            "Trained for 89000 times,\n",
            "Trained for 90000 times,\n",
            "Accuarcy Test:  0.9677\n",
            "Trained for 18 epoch(s), loss = 0.09272458201693769, test = 0.9677\n",
            "Trained for 91000 times,\n",
            "Trained for 92000 times,\n",
            "Trained for 93000 times,\n",
            "Trained for 94000 times,\n",
            "Trained for 95000 times,\n",
            "Accuarcy Test:  0.9657\n",
            "Trained for 19 epoch(s), loss = 0.10093461143380791, test = 0.9657\n",
            "Trained for 96000 times,\n",
            "Trained for 97000 times,\n",
            "Trained for 98000 times,\n",
            "Trained for 99000 times,\n",
            "Trained for 100000 times,\n",
            "Accuarcy Test:  0.9655\n",
            "Trained for 20 epoch(s), loss = 0.09669707801246029, test = 0.9655\n",
            "Trained for 101000 times,\n",
            "Trained for 102000 times,\n",
            "Trained for 103000 times,\n",
            "Trained for 104000 times,\n",
            "Trained for 105000 times,\n",
            "Accuarcy Test:  0.9705\n",
            "Trained for 21 epoch(s), loss = 0.08750620751477924, test = 0.9705\n",
            "Trained for 106000 times,\n",
            "Trained for 107000 times,\n",
            "Trained for 108000 times,\n",
            "Trained for 109000 times,\n",
            "Trained for 110000 times,\n",
            "Accuarcy Test:  0.9674\n",
            "Trained for 22 epoch(s), loss = 0.08948819887595072, test = 0.9674\n",
            "Trained for 111000 times,\n",
            "Trained for 112000 times,\n",
            "Trained for 113000 times,\n",
            "Trained for 114000 times,\n",
            "Trained for 115000 times,\n",
            "Accuarcy Test:  0.969\n",
            "Trained for 23 epoch(s), loss = 0.08181740227547635, test = 0.969\n",
            "Trained for 116000 times,\n",
            "Trained for 117000 times,\n",
            "Trained for 118000 times,\n",
            "Trained for 119000 times,\n",
            "Trained for 120000 times,\n",
            "Accuarcy Test:  0.9697\n",
            "Trained for 24 epoch(s), loss = 0.08314684440198762, test = 0.9697\n",
            "Trained for 121000 times,\n",
            "Trained for 122000 times,\n",
            "Trained for 123000 times,\n",
            "Trained for 124000 times,\n",
            "Trained for 125000 times,\n",
            "Accuarcy Test:  0.9724\n",
            "Trained for 25 epoch(s), loss = 0.07503696836206668, test = 0.9724\n",
            "Trained for 126000 times,\n",
            "Trained for 127000 times,\n",
            "Trained for 128000 times,\n",
            "Trained for 129000 times,\n",
            "Trained for 130000 times,\n",
            "Accuarcy Test:  0.9703\n",
            "Trained for 26 epoch(s), loss = 0.08209542408990533, test = 0.9703\n",
            "Trained for 131000 times,\n",
            "Trained for 132000 times,\n",
            "Trained for 133000 times,\n",
            "Trained for 134000 times,\n",
            "Trained for 135000 times,\n",
            "Accuarcy Test:  0.9721\n",
            "Trained for 27 epoch(s), loss = 0.07307681827983226, test = 0.9721\n",
            "Trained for 136000 times,\n",
            "Trained for 137000 times,\n",
            "Trained for 138000 times,\n",
            "Trained for 139000 times,\n",
            "Trained for 140000 times,\n",
            "Accuarcy Test:  0.9705\n",
            "Trained for 28 epoch(s), loss = 0.07455028740840566, test = 0.9705\n",
            "Trained for 141000 times,\n",
            "Trained for 142000 times,\n",
            "Trained for 143000 times,\n",
            "Trained for 144000 times,\n",
            "Trained for 145000 times,\n",
            "Accuarcy Test:  0.9673\n",
            "Trained for 29 epoch(s), loss = 0.08177282590037513, test = 0.9673\n",
            "Trained for 146000 times,\n",
            "Trained for 147000 times,\n",
            "Trained for 148000 times,\n",
            "Trained for 149000 times,\n",
            "Trained for 150000 times,\n",
            "Accuarcy Test:  0.9719\n",
            "Trained for 30 epoch(s), loss = 0.07108286187646136, test = 0.9719\n",
            "Trained for 151000 times,\n",
            "Trained for 152000 times,\n",
            "Trained for 153000 times,\n",
            "Trained for 154000 times,\n",
            "Trained for 155000 times,\n",
            "Accuarcy Test:  0.9716\n",
            "Trained for 31 epoch(s), loss = 0.07151303282767038, test = 0.9716\n",
            "Trained for 156000 times,\n",
            "Trained for 157000 times,\n",
            "Trained for 158000 times,\n",
            "Trained for 159000 times,\n",
            "Trained for 160000 times,\n",
            "Accuarcy Test:  0.9691\n",
            "Trained for 32 epoch(s), loss = 0.07738183730840653, test = 0.9691\n",
            "Trained for 161000 times,\n",
            "Trained for 162000 times,\n",
            "Trained for 163000 times,\n",
            "Trained for 164000 times,\n",
            "Trained for 165000 times,\n",
            "Accuarcy Test:  0.9694\n",
            "Trained for 33 epoch(s), loss = 0.07802235740867866, test = 0.9694\n",
            "Trained for 166000 times,\n",
            "Trained for 167000 times,\n",
            "Trained for 168000 times,\n",
            "Trained for 169000 times,\n",
            "Trained for 170000 times,\n",
            "Accuarcy Test:  0.9767\n",
            "Trained for 34 epoch(s), loss = 0.060459485174277655, test = 0.9767\n",
            "Trained for 171000 times,\n",
            "Trained for 172000 times,\n",
            "Trained for 173000 times,\n",
            "Trained for 174000 times,\n",
            "Trained for 175000 times,\n",
            "Accuarcy Test:  0.9741\n",
            "Trained for 35 epoch(s), loss = 0.06344818695617108, test = 0.9741\n",
            "Trained for 176000 times,\n",
            "Trained for 177000 times,\n",
            "Trained for 178000 times,\n",
            "Trained for 179000 times,\n",
            "Trained for 180000 times,\n",
            "Accuarcy Test:  0.9749\n",
            "Trained for 36 epoch(s), loss = 0.062301593020511845, test = 0.9749\n",
            "Trained for 181000 times,\n",
            "Trained for 182000 times,\n",
            "Trained for 183000 times,\n",
            "Trained for 184000 times,\n",
            "Trained for 185000 times,\n",
            "Accuarcy Test:  0.9739\n",
            "Trained for 37 epoch(s), loss = 0.05976329395377644, test = 0.9739\n",
            "Trained for 186000 times,\n",
            "Trained for 187000 times,\n",
            "Trained for 188000 times,\n",
            "Trained for 189000 times,\n",
            "Trained for 190000 times,\n",
            "Accuarcy Test:  0.9745\n",
            "Trained for 38 epoch(s), loss = 0.062020440195180514, test = 0.9745\n",
            "Trained for 191000 times,\n",
            "Trained for 192000 times,\n",
            "Trained for 193000 times,\n",
            "Trained for 194000 times,\n",
            "Trained for 195000 times,\n",
            "Accuarcy Test:  0.9749\n",
            "Trained for 39 epoch(s), loss = 0.05822341630071962, test = 0.9749\n",
            "Trained for 196000 times,\n",
            "Trained for 197000 times,\n",
            "Trained for 198000 times,\n",
            "Trained for 199000 times,\n",
            "Trained for 200000 times,\n",
            "Accuarcy Test:  0.975\n",
            "Trained for 40 epoch(s), loss = 0.05726429080290373, test = 0.975\n",
            "Training finished!\n",
            "Accuarcy Test:  0.975\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy, wrong_case = model.testing(x_test,y_test)"
      ],
      "metadata": {
        "id": "gXsDBBKRWnvd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69ddae11-ef61-49e8-9617-91c09ff03f08"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuarcy Test:  0.975\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:138: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dự đoán \n"
      ],
      "metadata": {
        "id": "_ldxUPdq8DC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(x_test[11])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bML4r4U_xzSd",
        "outputId": "e6f99b3d-a7a6-4852-b3f7-9b8ca9503f25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test[11]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDi3zmGi4r4k",
        "outputId": "3b7c0390-14f7-4690-ac74-e5f205627f6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv2_imshow(X_test[11])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 45
        },
        "id": "ptTzjMps45VV",
        "outputId": "f5a165cb-30cf-4e83-b77f-da4c7653aeac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=28x28 at 0x7F0B14471290>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABIElEQVR4nM2QsUtCURTGfy+EQHgguPQeLxqK5lpC/weXWlyDiGiocGgJgyBobSsoW1uLmgoJAoUHluBQSEKgZUFDS0Jk53Jb8ul94hh0psP3u9893znw1zV8o05NZajLdqf07SDjulwmQlLXOULeHwjt7/zAOK4q9GmRTpMNpKRH5cF8VVfzAOw3PkTeNw0WbTaAyExT6beTun4e64WrsgfulsjTjgdnkuidOU0Nskv6KnMH1MxALjCZ5nCtDUC53Atty2Ildrzc2bkN3SNorXG08/vNwouRtigZ4q+fG3EAv2Vc2X2UDDglubZJ+V/mmlyo8yiQGh89UK0Qw6uowlwyuV0UdT9LuJyqiCgRycUDzQq6WHpiMac5qvb5/k/9AF+bZJFBxVz7AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize một số trường hợp sai"
      ],
      "metadata": {
        "id": "OCvo_SAn7-O3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig, axes = plt.subplots(ncols=10, sharex=False, \n",
        "    sharey=True, figsize=(10, 4))\n",
        "for i in range(10):\n",
        "    axes[i].set_title((y_test[wrong_case[i]], model.predict(x_test[wrong_case[i]])))\n",
        "    axes[i].imshow(X_train[wrong_case[i]], cmap='gray')\n",
        "    axes[i].get_xaxis().set_visible(False)\n",
        "    axes[i].get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "Iy9AB_AeDCyc",
        "outputId": "7f8616e9-6013-41f3-91bf-32c282a925eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x288 with 10 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAj8AAABRCAYAAAA9zcc0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO29eXRb13no+zuYiIkEQYIECc7zPIiaKMmyZMuR67qyUztOHLd9N4nbuLlNem+bvPjmZd3XdjW5N7drNWneS5u0zVtxbtN4iIfGs2MnHmTRMkVRJEVSnGeC8wiQAAkC5/0BnR1SlmxJlghAOr+1uGzhHG7sj3uffb797W+QZFlGRUVFRUVFReVmQRPpDqioqKioqKiobCeq8qOioqKioqJyU6EqPyoqKioqKio3Faryo6KioqKionJToSo/KioqKioqKjcVqvKjoqKioqKiclNxzZUfSZL+pyRJ//Vat3uR7/mKJEn/63p/zyW+W5Xx2n2PKuP1+97tku+YJElPXu/vucR339BjeP67VRmv3fdEZK6qY3hNv+fayCjL8jX7AVKAccB0kWv/NyADd1xBe8eAdsALNADlm64ZgTEg9VrKsJ0yAsXAL4EZYB54DSi5kWSMpXEEzMA/AbPAEvDOFbSXC7wJrAJdm/8+kZDxQvmAeuD18/NsBvgFkH6ZbWWfH7vNPzLw1U33tAPVkRzD82MgX9DP/36ZbTmAE8AcsAi8BxyIxnm66doN+SwC5UATsHD+543N/Yy1uXqjrzWxOk+vteXnc8DLsiz7Nn8oSVIB8AAwcbkNSZJUBPw78KdAIvAC8LwkSToAWZb9wCvA/3FNen75fI5rJCNhuZ4HSgAn0EhYGQJuDBljbBz/BUgCys7/9y+uoL3HgTNAMvBN4GlJklIgYjJ+jq3y2QnLlwvkAB7gJ5fTkCzLI7IsW5UfoAoIAc9suu1x4IvXpuuXzee4yDwFEjf1928vsy0v8AXCi7gd+F/AC1E6T2/0Z9ENfIrwM+ggvEY+cTkNRelc/Rw39loDsThPr7H29xvgDy/y+avA7wJDXL5V5MvAS5v+rQF8wJFNn/0B8OY2a7jXTMaLtJFEWENOvlFkjJVxBEqBZSDhKtoqBtaA+E2fHQf+NFIyXmoMN12vAzxX2fZfXSgLcAAYjPAY5p5/fnQfs10N4Z2nzKbdZTTM002f37DP4gXXdMCfAatX2XbE5+qNvtZ82BhG8zy91pafKqB78weSJD0ArMmy/PJVtCdd8P8SULnps3NAzVW0+3G41jJu5lZgUpbluU2f3QgyxsI47gGGgb+RJGlWkqSzkiTdf5ltVQADsix7Nn3Wev5zhe2W8QNjeAG3Ah1X2qgkSRLhHddPL7h0DsiVJCnhStv8GFxKxmFJksYkSfqJJEmOK2lQkqQ2wE/Y2vBjWZanN12Ohnl6MzyLAEiStEh4LP5f4H9caaNRNFdv9LUGYnCeXmvlJ5GwOR0ASZLiCU/a/3IVbb0BHJIk6bAkSQbg/wIMhM9KFTyA7eq7e1VcSxkFkiRlAv8I/OUFl2JdxpgYRyCT8MO1BLgI7z5+KklS2WW0ZT3/e5tZAuI3/Xu7ZbxQPoEkSdWEz+H/z6to9xbCR7RPX/C58l2JV9Hm1XKhjLPAbsLHejsJ//3//UoalGW5GkgAHgLeveByxOfpTfIsAiDLcuL5vnyZ8DHPlRItc/VGX2sgBufptVZ+Ftg6CH8N/Jssy0NX2pAsy13AfwJ+QPi80AF0EnZ0UojngxPhenPNZFQ4f177K+CfZFl+/ILLMS1jDI2jDwgA35JleV2W5bcJOxUevYy2vIRfmJtJYOuCt90yXigfAJIkFRI+L/8vsiwfv4p2/xPwjCzL3gs+V75r8SravFq2yCjLsleW5SZZljdkWZ4i/FI5en4hvmxkWfaffw7/myRJm3eX0TBP/5ob/1kUyLK8AvwI+N+SJKVeYbvRMldv9LUGYnCeXmvlp43wmaTCEeDPJUmalCRpEsgCnpIk6dHLaUyW5adlWa6UZTmZ8NltLnBq0y1lhE1+28k1lVGSJDthxed5WZa/fZFbYl7GGBnHtovcI19mWx1A/gUv2Rq2Hittt4wXyockSTmEd1V/K8vyv11pg5IkmQg7L154jABh+YZkWV6+ir5eLR+Q8QKU8bvadU4P5G/6dzTM05vhWbwQDeFdf8blNhhlc/VGX2sgBufptVZ+XgYObfr3EcLmvdrzP27gEcLHO0iS9NeSJL11qcYkSdopSZL2vGXkXwgrCF2bbjlEeBe7nVwzGc+fOb8GnJBl+b9d4vtiWsbz12NhHN8BRoBvSJKkkyTpAHAb4fFBkqTPSZI0dLGGZFnuAVqAv5IkyShJ0u8D1WyNMNluGbfIJ0lSBmGnxB/IsvyjC2/+MPk28fuEd3hvXuRaxMdQkqS9kiSVSJKkkSQpGfh/gLdkWV46f/3DnsV6SZJukSTJIEmS6fwi7QTe33RbxGXkJngWJUn6hCRJO873MwH4LuF5d+789Vibqzf6WgOxOE+vsce3g7Bp6gOx/uevD7E1J8H/B3z7Q9p7l7A5bx74Z8Cy6ZoS6++8ljJsp4yETXsysMLWvBTZN4qMsTSOhJ0G3zs/Hp3A72+69t+Bf/+Q9nKBtwibtLu5eO6NbZPxQvkI757kC+aZ93LlO3/Pa4StRhe7dhaoieQYAp8FBs+P3wTwv4G0y5mnhBfT1k3z9G3g1micpxdcv+GeRcIWm67zc3QGeIlNeXliba7e6GtNrM7T6/FH+B/Af73Me1vYFNZ9hd/zFeDvtnOAVRlvahl/BZTFkozbKN8x4KkYGMObYZ7eDDLG3Fy90deaq5Ax4vNUOt+YioqKioqKispNgVrYVEVFRUVFReWmQlV+VFRUVFRUVG4qVOVHRUVFRUVF5aZCVX5UVFRUVFRUbip0V3KzJEkx7R0ty7L0YddjXT5gVpbllA+7IdZl/KgxBFXGWEB9FmNfRnWehrnRZYx1+bjEs6hafm4shiPdARUVFUB9FlVUooWLPouq8qOioqKioqJyU3FFx14qKioqKirRiFarRafTodFokGUZv98f6S6pRDGq8qOioqKiEvPcfffdHDx4kN27dzM6Osof//Efs7a2FuluqUQpqvKjoqJyUXQ6HSaTCYPBgMFgIDk5GYPBgCRJ+Hw+PB4Pi4uLrK2tsb6+HunuqpxHGbOUlBQMBgMmk4mlpSW8Xi+zs7NsbGxEuovXBI1Gg8PhwGQyER8fT01NDbW1tdTW1pKcnExVVRXz8/MsLy/j9XoJBAIEg8FId1vlMkhOTsZsNpOSksLKygpLS0ssLCxcU2VWVX5UVFQuSmJiIpWVlWRlZeFyufjc5z6Hy+VCr9fT1tbGu+++y0svvUR/fz/j4+PqiyVKyM/PJzs7m4cffpjc3FzKy8v51a9+RUNDAz/96U+ZmpqKdBevCWazmQceeICKigqOHDlCQkICZrMZk8lEQUEBjz/+OI2Njbz++us0NjbidrtZWlpCLekU3eh0Oo4ePUpdXR2PPPIITU1NvPrqqzz77LP09fVdu++5Zi1tM3FxcezevZuUlBSys7M5ffo0vb29zM3N3TA7G5UbA51OhySFo0mDwSChUCjCPfotcXFxZGVlYbFYsFgs5OTkEB8fTyAQYGNjg/X1daqrqyktLSU1NRWz2YxWqyUjI4Pdu3cjSRKDg4O8+OKLLCwssLS0FGmRblrS0tLIzc3l8OHDFBQUUFJSIiwjxcXFaLVaent76e3tpbOzM6rm4ZUSFxdHYmIitbW1FBUV4XA4MBgM6PV6NBoNOp0Oh8NBeXk5kiTh9XrFf2Pp/aDRaHA6nVgsFhITExkbG2N2dpaCggLi4+NJSkoiKSkJp9PJyMgIc3NznDt3jtXVVVZWViLd/SsmLi4Oi8VCeXk5JSUlmEwm4uLi0Gq11/y7Ylb5MZlM3HvvvdTU1HDkyBG+973v8cQTT+DxeKJ+cisvws2ou5EbE41Gg9FoRJIkJEnC7/dH1RGR2Wxmx44dOJ1OXC4XR48eJSsrC6/XS39/P6+88gp79uzhlltuEb8jyzIulwuXy0VpaSnj4+N0d3czMDAQU8rPhc9hLD+DkiSRm5vLXXfdxYMPPkhxcfGW62VlZRQWFuJ2u0lNTaWrqyumlR+LxYLD4aC+vp6MjAwSEhK2XNdqtSQkJFBdXU1VVRVjY2Osr6/T398f9e+Hzej1evLy8khPT6e4uJg333wTj8dDXV0dOTk5lJWVUVZWxs6dO3njjTfo6OjA6/UyOTkZk8qP2WwmKSmJmpoaKisrkSTpus3TmFJ+9Ho9RqORiooK8vLyuP322zEYDDQ0NNDd3c3U1BSBQCDS3RQYDAaMRiP19fWkp6cDkJSURGlpqTDRSpLE5OQkjz/+OMPDwwwMDIjf12g0YoEOhUIxvTjfqGg0GrRaLQaDAY1ma+aI+vp68vPzOXLkCCaTCY1Gw49+9CNeeOGFCPV2K0lJSRQXF/Pwww+TlJSEzWYjJSUFk8mE2WxGo9EQCASQZZlTp04BYStWfHw8iYmJOBwO9Ho9er0+wpJcPjk5OTidTo4ePYrT6SQnJweAtbU1Hn300S3PX6yQkZHBn/zJn1BeXk5NTQ0ul+ui9+l0Ou68804cDgePPfZYTCkBF/KZz3yGffv24XK5MJvNH3n/0aNHKSgowO12i59oxeFwkJmZSX19PTk5OdTV1REfH4/NZuPIkSMsLCyQnZ2N2WwmISEBq9UKQE1NDbm5uWRnZ3PmzBkee+wxFhcXY0oJOnToELfddhuVlZWkpKRc1FBwrYgp5cdgMBAfH09RURHl5eVkZWWxuLhIT08PExMTeDyeqNjNSJIkzLIOh4Oamhqys7ORZRmHw0FlZSWJiYnEx8djMpkYGxvjzJkz+Hy+LYuvXq9Hp9NhMBhYXV1VIxciiCRJW5RPxbRusVgwmUwkJCSg0219nGpqaqioqODOO+/EYrGg1Wp55ZVXtrvrF0WSJIxGIzabjcLCQpKTk0lISCAUChEMBsWL0Ww2Mzs7y/LyMhA2S2dkZAhnU+XvcD0XqWuBsnHKzc0lLy+PQ4cOkZWVRUlJCQCrq6t8+9vfjnAvrxyz2YzT6eTQoUPk5uaSm5sLhDdLi4uL4ugyMTERq9VKTk4Os7OzH1DUYwFJksSGsqamhj179mC1WrcciYRCIVZXV5FlGYPBIMLf8/LyMJlMpKWl4fV6o1L5Ud4bTqeTkpIS6uvrKSoqoqqqSgQaOJ1OVldXkSQJrVYrNil+v18cgZnNZoLBIHa7Hb/fH1PKT2ZmJjU1NcLh+XoSU8qP0+mkqqqKz3/+89TW1iJJEt3d3fzwhz9kZGQkapzZEhISqK+v5+jRo3z6058WfhKLi4uMj4/zm9/8hoWFBfx+P/fccw+JiYn80R/9EXFxcTQ2NhIKhdDpdOTm5uJyuaitreXNN9+kpaUl0qLdlOj1egwGA36/n2AwiMFgwG63U1BQwKFDh6irq6O2tlbswBSMRiM6nU4sUHDxI89IsbGxweLiIidOnKCiooIdO3YwOTnJ7OwsL774IgMDA5w6dUpECgGkpKTwyU9+koMHD5Kfnx9hCS6fqqoqbr/9dj71qU9RXFwsnslYRqfTcf/997Njxw527dqFyWQS1xYXF3n00Ufp6+uju7ubb33rW3zhC1+IYG8/HhqNBovFQl1dHYcPH+bAgQPk5eVtGUOfz8fq6ipPPfUUq6urlJeXk5+fT0lJCWazmcTERFJSUpiZmYmgJJcmISGBW265hTvuuIP777+fhIQE4ce0srKC2+3m5MmTnD17llOnTmE0GrnnnntISUnB6XRSWVkp/H+ys7OprKzE7/czNzcXadGikogqPw6Hg4yMDAYHB8XO8mIoGn96ejrV1dWkpaVhMpno7u5maGgIt9vNyspKxBUfSZLIz88nIyODAwcOUFFRgdPpZHJykqWlJXp6ehgdHeX06dMsLy+ztraGw+HA5XJRWFiIVqsVviEAgUCA1dVVZmdnr2vCLuU7ldBmq9VKXFycsGTo9XpMJhMbGxuEQiHW1tbY2NjA5/Ph8/lYWVkRx3I3UsSPJEno9XpSUlJIT0/H7/cTCoXEAlNWVkZtbS3FxcW4XK4tL5/N+P1+1tbW8Hg8eDyebZbi4siyzNraGktLS8LaGAgEGBoaYnp6mubmZiYmJsSztba2htFoBCA9PR2bzRbJ7l8xTqeT3bt3k5WVhd1uj3R3rgmSJGE2m7FarRiNRqEIdHZ2MjAwQGdnJ2NjY0xMTDA9Pc3s7CyLi4sMDQ1FhYX8SlDW/8LCQnbs2EFycjJ6vV6sOYFAgN7eXgYHBzl9+jQbGxvYbDaSk5PF+qYcUUfTBkTBYrHgdDrZu3cvZWVlOBwOdDodsiwzMzPD7Owsvb29nD17lrNnzzI0NIROp6OxsZGkpCQcDgfx8fHC18lsNpORkUFXV1ekRYtaIqr8VFVV8cADD/BP//RPtLe3X/I+nU5HUlIS1dXVPPDAA7hcLvx+P7/61a9oamqKGhOmTqfjnnvuoba2lnvvvRej0YhGo+HkyZO0trbyzDPPMD8/z/T0NBBevFpbWykuLuYb3/jGlmOtYDC4ZbG6ni9NxTRstVrJzMykuLiYtLQ0LBYLADabjfz8fDweD16vl7m5OTweD6Ojo4yOjtLf38/6+rpQiCKthF4rtFotiYmJVFdXc/vtt+Pz+dBqtVRXV5ORkUFtbe1HthEMBpmbm2NmZoaOjg7Gx8e3oeeXh2Ipfe+992hubkan03Hu3DlmZ2eZn5/f8oKUJImkpCTy8vK4++67SU5OjmDPr5ySkhI+/elPR7ob1x1ZlvnZz37G22+/TVNTk3Cun5iYoK2tjZaWFrq7u2Nuk2I2m6mrq+PQoUMcO3Zsy7GdYsF87rnneO655+jv7ychIUFsUhQ2byyjCUmSSE9Pp6amhkceeQSr1YrBYCAUCuHz+Th9+jRDQ0M0NjbS2NhId3e3+N2hoSH0ej1xcXGYzWZkWaaqqgqbzUZNTQ1tbW0RlCy6iYjyY7fbOXz4MPv37+fAgQOsrKzQ1dXF66+/zuLi4gesQFqtFrvdjtPpJD09HaPRyOLiIm+++Sa9vb2REOED2O12kpOT2b17NxUVFRiNRkZHR+nq6uLFF1+ks7OTqampLRYcWZZZWlpiamqKrq4ulpaWSE5OZnFxkfX1dfx+v9ixXC9Hbq1Wyyc+8Qmys7PZsWMHNpuNxMREzGazcGQ1GAzYbDbW19cJBAIiYsnr9eL1ellaWmJxcZHV1VXcbrdYWKenp5mfn2dkZASPx8PCwsJ1keF6oCg+Bw4coL6+niNHjrCxsUEwGGRlZYXx8XHa29vJy8sjIyNDODQDjI2N0dfXJ8awubmZxcVFZmdno24n5vf76erqQqPRoNFoWFhYwOfzbVF8FIvgww8/TFVVFQ6HA6PRiCzLLC8vMzMzw8jICLOzsxGU5OKkpqZy//33c+utt37ofXq9noceeoiSkhKefvrpmFAOtFotBw4cYMeOHWi1WlpaWnjnnXdoaGhgYGBgi0PzW2+9JcZoYWEhJuTbjMlkoqKiQgSOyLJMKBRieXmZvr4+Xn75Zd59910mJiaEwrd5EzYzM8PY2JiwhkUTitVdp9MRFxcn1l1lrXn11VcZGhqir6/vojmaNv++TqcjEAgwMzPDqVOnYiKnkyRJwhe2urqa7OxsDAYDa2trjIyM0NLSwokTJ675+2PblR9FkTl8+DB1dXVUVlYSCoXIzc2lpaWFQCDwAeVHp9Nht9uFQ5disj9z5gyTk5PbLcJFsdlsZGVlUVZWRn5+PhqNBrfbzYkTJ2hoaLhkcqa1tTW8Xi8TExMEAgEcDgerq6usr69vS0i0Vqtlz5491NXVcdddd6HRaD5yYVSi0DbvpGZmZsRCpChqfX19jIyMAGFFyOv1Rl2em0thNBpJSkqirq5O+PQoO7H333+f8fFx3n77bfbt2yccSpVjwvb2dt599138fj8ej4df//rXrK6uRuULZ319XYzRxVAcoxMTE/nd3/1dKisrMZvNhEIhNjY2WFhYYHp6munp6agLc9doNCQlJXHfffdt8U9S6j5pNBri4uKA8HNwxx13YLfbee6556JyrC5Eo9FQXl4uctn09fXx7LPP0t7e/gE/j7a2tpi2Auj1ejIzM7HZbASDQSRJIhgMMj8/T3d3N88//zyjo6PMzc1tOcZX6nwtLi4yMTEhjnWjDeVIbnOErzJPW1paGBoauqjVWKvVCquP4qqwvr7O3NzcRedBNKLT6UhOTmbPnj0UFRWRkpKCXq/H4/EwODhIT08P7e3t1/z0Y1uVH61Wy9GjR6msrOSOO+7A4XCIM82RkRFGR0c/oN0ZjUZcLhef+tSnqK2tRZZlxsbGPrCziTS7du3i9ttvJy0tjUAgQENDA6+++ipPPPEE8/PzF/0drVbL3XffLZy4lZ3JRx0DXku0Wi319fXU1NTg8/no6uri9OnTl7xfo9GQl5dHYmIimZmZGI1GjEYjCQkJJCQkkJKSInZce/bsYW1tjampKUZGRnjiiScYGBigp6cHn88XlS8YrVaL0Wjk05/+tDiWNZvN+P1+ent7GRoa4h/+4R+EH8W77767xeoDYWuK1+slFAoRCoWET1SsoVgdjx07xh133CEiZgBmZ2cZHR3lu9/9Lo2NjVGp+BQXF7Njxw527NhBfHy8uDY7O8sf/MEfUFlZyXe/+11xf2lpKV6vN2YioUKhEOfOncNsNlNaWhrp7lxXFhYWePLJJzl48KCILvR4PPzkJz+ht7eXrq4u1tfXkSSJ5ORkCgsLue+++8jKygLCmzO3243P54uq9waEx3FiYoKenh5+85vfUFRURElJCXq9ntTUVP7mb/6Gt956i29961tbfi8hIYE9e/ZQVVVFfX09u3btwm6309PTQ1tbG01NTVFf3NVoNHLbbbexZ88eHn74YWw2GyaTCUmSmJ6e5l/+5V/o6elhaWnpmq+h26b82O127HY7lZWVlJWVkZKSglarZX5+nrGxMUZGRlhdXf3A8Y6SJC4zM5PExEQA5ubmmJiYiKoXis1mw+VyERcXt8XPY3Jy8pI+MBqNhvz8fOEwu7KyIsIztwtZlvH5fCwtLYljmebm5kveL0kSCwsL2Gw2pqamsFgsxMfHY7FYMBgMWK1WcYSSkJAgrAYJCQnU1dUJP6L+/v6oe2FCeBwdDgdVVVVUVFSQlpYmnLwVK4fb7WZ2dlbUnIlGJe7jotFosFqtuFwuysvLqaysxGKxEAqF8Hq9DA8P09raKpxro4m4uDhR4iAnJ2dLmgKfz8fCwgJtbW3ExcXh8XgwGo3CqV/JvRULyLLM3Nwcs7OzyLIsxutalgCIFtbW1hgaGsLhcAg/Hq/Xy9mzZ5mcnGR1dRUIb17S0tLIyckhIyNDOObPzs7idrvZ2NiISp9Ev9/P/Pw8bW1tIoO6kt24pKSEwcFB9Ho9oVAISZJITU0lLS2N2tpaqqqqqKqqwmq1sra2JhKORktwxYeh0+nIzs4W47WZ9fV13G43i4uL12WN3Tbl5/Dhwxw+fJg77riDlJQU7HY7fX19tLS08OSTT3L27FkxgS9Er9eTkZEhlJ+2tjYaGxujKu+NyWTCZrOh1WpZX19neXn5I51/tVot+/fvZ8eOHRgMBgYHB/nlL3+5rUd5GxsbPPXUUzgcDtbX1+no6OD999//0N/ZHLatOBUqY1pfXy9y3+zevZuSkhLi4+OxWq189atfFf4wf/VXf8XJkye3Q8QrYvfu3Rw4cIBPfepTpKeno9VqCQQCrK2tCWVWiTTxer2icOKNhHLUtXPnTr72ta9RUlJCZmYmOp2O5eVlUdPrZz/7WVTuLDMzM8nNzeXBBx8kPT2d9vZ2MjMzyc/Pp7+/n56eHuEke+bMGQoLCy+ZGDCaCYVCdHd3Y7Va2bdvH+Xl5Xz+859ncHAwqhzrrwV+v5/u7m76+vr4xS9+AYSVv83KjEajwWAwcOzYMXbs2EFGRgZarRZZlmloaODNN9/E5/NFUoxL4vP5GBoa4nvf+x7Hjh3DarVSUVFBUlISaWlppKWlYbfb2djYQKfT8cUvfpGKigoOHjyI2WzGaDTS29vLwMAA3/3ud6MmCOijMBgM1NXVUVRUtO3ffd2UH51OJ+oEOZ1ODh48KHb+Xq+XtrY2zp07R3NzM0NDQywvL39AUVA0XJfLRVJSkrA6DAwM0NvbG3XmyyvZMSrJ1jIyMoiLi+Ps2bN0dnbS29u7rUmpFNO5yWQiFAoxOTl5RX9XxRzp8XgwmUz4fD7htDc8PEx+fj633347SUlJ6PV64uPjRRZhnU4XdWOo9G9z6LBOp8NoNFJYWCiO91ZWVvB4PPT39zM2NkZ7e7s46opl6urqcLlcIuqvoKAAu92OVqtlbm6OyclJOjo6GBsb+4BjdLRgt9vJzs4mNzeXtLQ0IPxiHBsbo6Wlhc7OTtbX15mfn6ehoQGLxSKUH2WjNTMzE5WWyc0Eg0FaW1vR6XSEQiFsNpvI8Ds8PMzU1FRUjs/Voig7l1ozlCS4ZWVlFBcXC/8ZxTk62taaCwkGg6yurrK4uMjMzIzY3EuSREZGBvfffz+BQABJkti9e7dwO/D7/SwsLPDOO+/Q0dHB1NSUyMsVCyj+TgqyLDMxMcH4+Dgej+e6bbCum/Kj1+upqqoiLy+Puro6du3aRVFREbOzs4yMjPDcc8/R0dHBmTNnWFlZuejE1Gg0ZGdnk5eXR2pqKmtra0xPT3Pu3Dk6OjqiqpTFZjbnlbgwM7Byrbq6mltuuYW8vDwkSaKhoYFTp05tezRQKBSitbX1qn9/dXWV1dVVEVWg+CpJkkRaWprIzWGxWESW3aSkJHFMFgwGo8oMbbVaRY4NBSWKoqamBoBPfOITIkdOY2MjZ8+exe12s7a2FlXWyCtFkiRuv/129u3bx86dO0lISBDHBopfwsDAAKdPn2Z4eDhqX6wpKSkUFRVRWFhIWlqa8BPs7e3l+PHjtLW1ibXk1Vdfpbi4mJ07dwJhH4SCggKAqFd+QqEQDQ0NrK2tEQwGsdlsJCQkUFBQwNjYGBhq1UoAACAASURBVHNzc1FVR+56YzKZsNvt1NbWUlFRseVatM7VzSiBPMvLy0xOTm5ZS/Ly8vjSl77E+vo6oVCI0tJSjEajWIeGh4d5/vnnaWpqYm5uLibkVbjQaBAKhRgaGqK3t5f5+fnrZq27LsqPVqvFZDJRVlZGeXk5hw4dwm63ixpBnZ2dvPnmm6yurmIymUROA0UBUhQHo9HI3XffTXV1NSaTieXlZSYmJpibm4uabM4KGxsb+P1+ZFkmISGBo0ePkpiYiM1mo6GhgcnJSSwWC1lZWRw4cID9+/dTUVFBYmIiExMTtLa2xpSpWq/Xk5ycTHJyMqmpqfT19bG4uIhGo6GgoIC7775bWElcLpcI39Tr9VgsFhG5NzExEVU+M11dXRgMBpKSkoiPj2doaAi73S4sCDqdjoyMDHQ6HTabjd27d1NUVMTGxgadnZ08//zzrK+vR5VMl4NSNbqgoICioiKSkpJEJBSENyK5ubnEx8cLK9/AwIB48UYDCQkJZGdnc/ToUe6++27sdjtjY2P8+Mc/FlEjExMTLC8vi6jS9vZ22tvbKS0tJT8/H5PJRHl5OSaTSSR13NjYoK+vL2otBz6fj46ODjIyMkhPT+fQoUM4HA6CwSATExMfGs13I6BYlI8dO8att95KWlqa2GQqwTEdHR1RFyRzKXp7e3n22WfZsWOHKFdiNpvJzs4WyWQlSWJ8fJznnnuO7u5uWltbRWHhWFF8Dh8+TEVFBfv37yc1NRWA4eFhRkdH+cEPfkBPTw8LCwvXbcyui/ITFxeH1WolKyuLrKwsMjIyRGji4uIic3NzrK6uEhcXh8PhEL+32byl1E0qLy+noKAAvV7PxsYGXq8Xn8/H+vp6VCk/KysrzM3Nsby8jN1uJzMzk+XlZZaXl5mbmxMRUYWFhezfv19YxZQszqOjoywuLkZajMvCaDRitVrJzc0lPT2dnJwcZFkWNYPKyso4dOgQcXFxGI1GkXkUELWjlJpQ0cb09DQ9PT10d3djsVjo7OwkNTVVWAH0ej2SJAkFzm63Y7PZqKurQ6fTiarL0epbcCmUKDflpa/VagkGg/h8PlFjzmq1IkkShYWFtLe3YzQaCQQCUaP8WK1WSktLKSkpET4Eq6urnDx5kv7+/g84AishwZOTk7jdbrKysjAYDOTl5QHhY4jl5WW8Xm9UO0H7/X56enrQ6/XiedRoNLS1tSFJ0g2h/GwOXVfScSgbZqPRSFpaGlVVVezbtw+LxSIyP8/NzdHV1cXk5GTM+OZ5vV5GR0e3rCFKQWEIr6Hj4+MMDg7S2NhIZ2dnTKUxUNwIysrKqKurIyMjQwTCzM7O0tfXJxI7Xk9F7rq8fcrLyyktLeX3fu/3hHKjKColJSWkp6dz8OBBUlNTycrKEmXrN/v9KIvw5oKRwWBQmHG1Wm1UafGvv/46ra2tdHd3U1RUxO233y6cLO+9915RLFLJpqxEzvT09NDS0iJKXkQ7Wq2Ww4cPU11dzZe//GXhu7PZAqBEfW3Ot6G8PMbGxmhubqa9vZ3R0dGoeXEqDA8PMz4+TktLC5IksbGxseVM2mAwiJfrnXfeSVpaGg6HgwMHDpCbm8vZs2fp6uqis7NTVESPBTweD6urq5w5cwatVkt5eTmLi4u0trZSUlIifGiUGksOh4OsrCwGBwej5vi5pqaGH/3oR2IhvVwUX6CqqipSUlJE2one3l6+853v0NbWFjUyXozh4WG+8pWv8Mgjj1BTU0NhYSF5eXns2rWLF198MSoDC64Uk8lEcXGxsCYryszAwAAlJSX85V/+JTU1NeTn54v1yO128+abb/KTn/yEoaGhSItw2aSnp7Nr1y6SkpIuet3v9/P1r3+d1tZWRkdHo3puXozi4mL27t3LQw89RHV1NRaLRbwfmpub+eUvf8ns7Ox1t2Bdt623JEmsra2xvr4uKtIqmRyVF6NSaE5RfuLi4oTAFotFmJ1lWSYQCLC0tITb7RY1lqKJ5eVlgsEgLS0tzM/Po9FoSExMFHWEFGc9pYaLItvS0hJLS0sXDfOPRpRIIKvVKpJRKeOnEAwGWVtbE0m3Nu+alUKfSnixYsa93n3Oz88XBQI9Hs8lrWzBYFD0/2JotVoMBgOBQACLxUJ2djYul4sjR46QlJTE/v37CQaD9PX1ibZiAWV+dnV1sbGxgdvtFg7dSkh/amoqVquVxMREkpOTSUtLi4qoEoPBQG1tLbW1taLshizLDA8P09fXx8zMzIc6gCrrSiAQECH+yg8Q9c+lkuxvfHyc7u5uMjIyRH2+3NxcbrnlFiYnJ1lYWLhuYcMfF8XVIT4+HpvNRk5OzpbrZrNZRI7Gx8eLpJoZGRniyNJut4uNshIm7Xa7mZycjMqoRAVJkkQQiNVqpbq6mr1792K327esnZv9RxcWFsQJSqyg1OhMSUmhuLiYlJQUofgoVubFxUXm5+e3ZY5eF+XH7/ezuLgosv0WFxcDYeGVI5KLcanPg8EgHo+HoaEhTp48yfT0dNQtSIrj7/PPP09cXBwvvPACKSkpwlcEwvIVFRVx//33k5+fj9FoZH5+Xnj2R+Oi9GHIsrylmKmi4Pp8PsbGxkhNTd2yIAEkJiZSVFSEy+UiOTmZycnJ6y63Tqfj2LFj2O12+vv76ezspKmp6araCgaDjIyMMDIyQkNDA4WFheTn51NeXk5RURFf/vKXiYuL45133omqI6HL5bXXXuO1117b8pli6aqpqSExMZGsrCyKioqoqqoSykUkSUhI4Otf/zolJSXis42NDX71q19x+vRp2traPtRK7Ha7RSLLWKanp4cnn3ySBx98UKy5ZWVl/O3f/i2vvPIKDQ0NtLS0RGUkkHLsqhQu/dznPrflelxcHJmZmZjNZsxmM4uLi6ysrDAxMSH8YTb7qK2urtLc3ExPT09UZnTejEajoa6ujszMTEpKSti5cyeHDh3ack+01iW7EpQanYWFhRw4cEAUnYWw35rb7WZqauq6+vls6c/1aNTtduP1evnxj38sanKlpqYKS4/i/KrQ3t7O7Owsq6urxMfHk5WVRX19PWVlZWxsbDA5OcmTTz4pslZGexRGIBBgfn5eVGSH8MAXFhYKX5mEhARCoRCdnZ2cO3cu6ixZlyIYDNLW1sbMzAwbGxsiOgF+O7mV2lfKQnXw4EEyMjKorq5Gr9fjdDr5/Oc/z6FDh3j++ecZHx8XqQuuh7KgZBGPj4/n7rvvpq6ujn379vHCCy98bHO4ktxSqXW1sLCA1+uN2mRqV4pGoyE9PZ2SkhJhrYQrS+twPdm5cyfFxcWUlZVt2WhcCYrytln5UZT0Kz1CiyRDQ0O8/PLL2Gw2xsfH2b9/P1arlcLCQu69917q6ur44Q9/yMDAAGNjY1E1P202G2lpaTzyyCPk5eWRl5fHxsYGgUCAjo4OJEnC6XSKddJsNmMwGNDr9aKw5+Zw6ZWVFZqamhgeHo6USJdFSUkJ+fn5fOYznxEpXRTn381E01hdKRqNhurqatLT09m9ezfV1dXk5eVhsVhYW1vjxIkTjI2N0dbWxvvvv8/09HTsKj/z8/MsLCwwOTlJXFwcFouF/Px8srKyKCws3LKIArz88ssMDg6ysbFBWloae/fuJS8vj7KyMlEe4eWXX2ZoaCjqJzMg8t54PB6xsBqNRrGYKjlF/H4/g4ODDA4OxszklmWZgYEBxsfHmZycFJmhIbx7y8rKYm1tjYmJCSDsILy+vk5lZSXZ2dnCrH3nnXeyvLzM9PQ07e3tjI+PX7f6V7IsMz8/T2ZmJnv27GF1dZUdO3Zw9uxZJiYmhKJypQqoRqPBbreTnp4uIhaVI8xYqWH2YShO6SkpKeTm5mIwGACu6m91vVAsBU6nE6vVKo6uQqGQKMT7USwtLREIBLbca7VasVgsosJ2LISMT05OMjk5SXZ2Nn6/X1T3zszMJDMzk7W1NY4fP87a2hrj4+NRseZIkiTqPWZlZXHPPfeQnJwsMs8rG0ilyKdi3YmLixPvlovJoZSjibRV8mIoVhytVkt+fj779+8XpZE2z13lGYuLi/tAypRYQik1U1ZWxrFjx3A6neId6PF4OH36NF1dXTQ0NDA9Pb1tgT/XzedHyVmgRDMtLy/T0dGB0Wj8wK5xaWmJ9fV1dDod5eXl3HfffRQWFhIKhTh58iQtLS20tbVta/K/a01cXBz79u2jsrISCA/6/Pw8w8PDTExMxNzEXl9fZ2xsbIuyEgwGcbvdW2TZ2NjgmWee4a233uLkyZPs2rWLQ4cOkZ+fj8Vi4Ytf/CJtbW3o9XqampquS2p+xcIG4UiKxMREXC4X3/jGNxgYGOCXv/wlIyMj4p7LwW63U1RUxH333cddd91FWloaKysrnD59mr6+PjweT8wdeV1IYmIipaWllJeXi/EKBoOMj48zNTUllIZI0t/fj9/v59y5czgcDgoKCoSS/cQTTwgfpqtBkiT+9E//lH379vHtb3876i3OCm+88QanTp2io6OD+vp6/vN//s9A2Dfq0Ucfpampic9+9rNRkZOquLiYuro6brvtNgoLC4mPj2dpaYmBgQFOnjy5pUjwz3/+c+677z6+9KUvfWS7er2etLS0qBszrVZLamoqGRkZ7Nu3j9tuu426ujocDofInN/S0sJvfvMburq6kCSJv/u7v7uk83MsoPg0ZWZmfsD4sbGxwfDwMENDQ4yMjGzrenJdY40Vf5BgMPiRQinHIZmZmeL81uv1cu7cOc6dO4fH44mq6K4rQam8m52dLUyaS0tLTExMsLS0xMrKSswoP0pJC71eL5zUNvsQXLhDVkLglV2bXq/HZrMRFxdHRkYGaWlpeL1eamtrGRkZuS7KjyzLLC0tMT09TVdXFwUFBSQmJlJYWEhiYiIjIyPEx8ezvLwswtQvFaklSRJWqxWn00lZWRn5+fm4XC50Op3IPj49PR0xxUdxKrTb7TgcDqamplhZWfnIUisXtmGxWEhJSaGwsJDMzExSUlJEXa/R0VHGxsaYmpqK+At0YWFBJDFMTExkaWmJmZkZbDYbIyMjl1XVWknNcbGaekr4f7Qc810OytFra2srZrOZ7u5unE6n8NdaWlqipKSEiYmJiFtG7HY7paWllJaWkpOTg9/vZ3Z2Vvjmtba2MjY2hkajITMzE9jq/6LkvLkQ5Qg+2o4tJUkSVq4dO3ZQWFhIeno6EF47Fd9WJXJYp9OxsrJCfHz8B9xFYgGdTofJZCI1NZXU1FTMZrN4zjwej6iBOT8/v+0+d1GRaEWn05GYmMjDDz9MTU0NRUVFuN1u+vr6+Nd//Ve6u7tjVvGBcORaamoqBw8eFBO9o6ODEydOMDw8HDP5fSCsyCkpDFZWVmhra6OhoeEjf8/n89He3k5vby8vvPACX/nKV6ivr+fQoUMUFhbyF3/xFwwPD19WW1fD4uIi7e3tfO1rX+O+++7jkUceETlRKisr6e/v58UXX+TEiRO0tbUxOTl5UYXdYDCwc+dOamtr+exnP0tOTo5YlBYWFnj++ecjmlfFYDCQlZXFsWPH+MIXvsAPfvADTp06xdmzZy97V6XX66mpqaGyspL77ruP4uJiMjIyGBkZYXR0lOeee47m5mbee++9iB9/DQwMiJefJEm8/fbb4uV4ufIqKSmUivUKsizzxhtviISssUQgEKC9vZ2pqSkGBwf5sz/7M+655x4AsrOz+fu//3uefvpp/vmf/zmi/czKyuL222+ntLQUi8VCS0sLXV1dvPTSS7S2tjI4OCgUpG9961uiSvtmhediSr3ZbKa6upq5uTlOnz69bfJ8FHq9nrKyMvbv389DDz2EVqsVFern5uZ45513ePvtt3nttdcIBAIkJyczOjqKVquNyfpzSUlJIvFmWVnZlg1Gc3MzHR0dNDU1CdeJ7SQqlJ/k5GQyMjKora0lOzub1dVVmpqaeP/995mdnY1pxUej0bB3716qq6tFkipFu29vb4/JZHhVVVVkZWUxPz+PJEl4vV5RWVkxaSoJyEKhEImJiaKshcFgECUEXC6XePiV5GXXk0AgwMLCAi0tLfziF7/gwIEDZGVlkZ2djdPppL6+nsTERCorK2ltbWVxcRGv1yuScmVnZ5OUlERZWZkIcVfyNbW3twsfokhWUzYYDLhcLlwuF06nk507d6LX6xkbG2N5efkjd1ebd2pKgVCbzSZ8Ebxer0gaFw3HekrE4dVgNBpxOBxUVVVRWVnJysoK09PTpKSk0N/fT0tLCy0tLR843o0WlOMECDttX/h3UMarv7+f1tZW0tPTqaioIC4ujvz8fCorK6mvr6e7u5uFhYVt7bsyT7Ozs0lPT0ev1+Pz+WhpaaG/v5+pqSmcTqdIXpiXl0dmZqYot6KkPzl79qzIxaVEl5rNZkwmE1VVVUIBPnPmDNPT0xG3VMJva1kpUbCyLON2uxkYGOD111+np6eH1dVVQqEQPp+PwcFB4uLiYkr5MZlMJCcni5xTycnJmM1mAGGdPX78OC0tLcLtZbuJCuXH6XRSWFjI3r17MZlMzM3N8etf/5onn3wy6s5srwTFqe3o0aMcPnyYhIQE8QLu6emhqakp5vyYNBoNe/bsEWZzJRdKY2Mj09PTwllR8fUKBoMUFhaSkpJCSUkJBoMBk8nErl27yMvLE8dn21ESQkmk2djYKBaYXbt24XK5SElJ4fDhw+zevZuVlRVefvllxsfHGR0dFdmcDx8+TG5uLsnJyaLeF4SVKuVBdrvdEXWONRgM5OTkiFQCirXxrbfeIhgMfqTyszk7e05ODoWFhcBvM3Mr1r5YSMj5USiRUPv27WPv3r0ixNbhcNDS0sIPfvAD2tvbL+voLBJIkkR2djaAqOd0oQK0srJCf38/7733HpIkkZmZSXp6Ovn5+ezevZvZ2VkRoLKdKOWPCgsLcblcBINBlpaWROTPzMwMu3fvprKykrvvvpv09HQSExOB31p6fD4fb7/9Nqurq+h0Onbu3CnWGIvFwp49e3C5XJSWlvL9738fr9cb8coAFzuiC4VCDAwM0NzczNNPP73FYunz+ejs7MRkMon6c7GAxWKhqKiIXbt2UVFRQVpamlB+ZmdnaWtr46WXXuL06dMR21hEXPmRJIkHHniAw4cPY7VacbvdQvv1eDwRN6t/HBTze0ZGBgUFBeh0OjweD6Ojo6JGWaxZtYLBIA0NDaysrLBnzx6Sk5Oprq7mwQcfZG1tTRwBKf5esiyLIqZWq1XselJTU9HpdPh8Prq6uvjFL37BqVOntkUGv9/P3NwczzzzDCdOnKC/vx+n0ymK6KalpXHkyBH8fj8+n08kNnQ4HKIWnbKINTU10dHRwTPPPMPQ0NCWKI1IsLa2Rn9/P+Pj4yIxYUJCAt/85jcZGhqisbFRROstLi6K+Wez2XA4HGRkZOByufjsZz9LRkYGEHYS93q9nDx5Uig+0bCD/rhkZmby4IMPUltbS1ZWFo8++ih6vZ4f/vCHzM3N0d7eHtVKnk6n484776SgoIDk5GR6enpobm5meHiY+fn5LfXITp8+LRT5iooKjhw5EtG+K75pSqi64md27NgxEfySlZVFamoqTqdTRHn5fD48Hg+dnZ0MDQ3x1FNPiffE5OQkc3Nz3HHHHdjtdoxGIxkZGdhsNvLy8hgbG4u472hcXBwHDx6kqqoKCCsCMzMzPPHEE3R0dGxRBJSSMvX19aJcS6yglJqpr69n165d4tQDYGpqitOnTzM/Px/RtTKiyo/RaMRisVBZWUlNTQ0rKyu43W7a2tqYmpqKeCTJx0U5QrDZbMJcqyRzWlxcjMmkarIs09fXh9lspqamBqvVSnJyMlqtVhxbKVYCRUFQjraU35dlWTj3eb1eBgcHOX78+LZlC1ac8Ht7e3G73eLYdXl5WRxx2Ww2kpKSMBqNaDSaLTs2v99PIBDA5/PR3d1NU1MTXV1dTE9PR1xZ39jYEDv5hYUFUlNTiY+PZ9++fbhcLkKhkMi6raQhgHAl9JycHHJycsjMzKSsrIz4+HhhLVNq7gwNDcVkQs4LMRgMJCcnU1VVRU5OjsiNEwqFmJmZYWZmJmotPgqSJOFyuSgrK2PPnj20tLSIDNVTU1P4/X78fr84FvJ4POI4KdIBFpuPupX/V0rHaLVaURR6s8OyUhtyfHxc+A92d3eLyMqcnBzsdju7d+/GaDSKcHiNRoPZbBZKViTRarXk5OTgdDoBRLHujo4O+vr6tqwfyvtDCTiAsFO0z+eL+jxicXFxJCcnk56eLhzVIbw+LS4uitplkZQhosrPwYMHeeihh6irqyMQCPCd73yHjo4OkYsi1nG5XFRVVYmU+4FAgMHBQX7605/S1dUV4d5dHYFAgCeffJLXXnuNV155hZ07d3LrrbeSnp4ujsA8Hg9jY2MYDAYMBgNOp1PkiAkEAqytrdHY2CiUwL6+Pk6dOrXtyq4sy6ysrPDqq68K605aWhqpqanceeed5OXlsXfvXmw2mzC5+/1+3n77bTo6OnjhhReEiX5lZSXiig+EFc+VlRU6Ojp49tln+eQnPyki0pxOJzU1NUxOTjIzM8P4+LhQflwuF3l5eaIsgtVqZX19neXlZU6cOEFTUxNPP/101Pj6fBwMBgPV1dXs3LlT+ETJssw3v/lNBgYG+OpXvxoTz2coFOLcuXOYzWZ27dpFVVUVxcXFIlloIBBgdnaWnp4e8vLySE9Px2w2bzmyjRa0Wq04CgM+4AO4vr7O5OQkL730Ev/2b//G6Ogoy8vLrK6uihfoiRMn6OjoICEhgdLSUiorK1lYWMDtdtPd3R0Vc1er1ZKWliZC17u7u3n33XdFVOZmEhIScDgcOJ1OUSapo6OD9vZ2JiYmojJT94eh5Jfq6+vj7NmzEfWNhAgpP4qzW2FhIcXFxfh8PhEmPDY2FnN+MJciISGB7OxsTCYTGxsborL0yMhIxAf+47CysiIsJ0qF5ZSUFOHs7PP5mJ6eFtXAk5KSxGKrZG09d+4cc3NzeL3eiIZMK8nUAOHUqyTecrvdLC0tYbVahdl2bW2N5uZmBgcHGRgYEE7R0bILC4VCrK6u4na76ezspLS0FICcnByhjIZCIcxmM1arVRwBKJlllYRqPp+Pqakpzpw5Q1NTE2fPno1IOOr1QqfTYTAYxHGKsjE5d+4c3d3dUV8SAcJj3dvbi16vp6urS4QTb0axnCjZg6OFYDDI8vIyXq8Xn88nLDQX1gJUKpjPz8/T0dFBW1sbo6OjzM/Pf2DNUCwJp06dYmpqitHRUTweD7Ozs4yPj0fc0gBheaanp8Uxs5IhPjk5WRyzW61WbDYbxcXFFBQUYDabRZHlgYEBTp06xdLSUsQVuUthMBiIj48XASEKgUBA1Fqbn5+PeOLQiCg/NpuN3/md3+HWW2+lurqahoYGuru7OXfuXMTzTlxLUlNTqa2txWaz4fP5eOWVV0TysVhGecH29fXR19fHSy+9FOkuXRNCoRCLi4vCLKuY4jfvQpXknaFQKCosPReysbHB3Nwc6+vrTE9Po9FoqKys5Atf+IKwzCnHsIpPz2ZCoRCBQIDp6WkaGhr4zne+IxarG5lAIMA//uM/0tLSEumuXDbBYJDXXnuNs2fPYrFYuOWWW7jzzju33BMfHy8Sq0YTa2trDA8PC8upUtjzQoLBIMePH6ejo4P/+I//EHlhLobikP/4449f7+5fNevr65w+fZpQKERhYSGVlZW4XC6ampowm8243W4KCwupra3l6NGjFBQUYLVaRbDC8ePHeeyxx/D5fFG5/ijFaTMzM6mvrxepXSC8aW5ububcuXOMjY1FsJdhtl35URLE1dXVicKWx48fF6GIsZZP48NISUlh586d2Gw2QqEQc3NzUe1AqbIVRRG4MKma4sgdzfj9fubn52ltbWV+fh6HwyGOtgKBABsbGyL5psPhwGAwoNPpaGlpYXh4mNdee42hoSEmJiZuqGdy586dZGVlsXPnTioqKoBwmPjk5GTEd6JXy9LSEq+++qqIoElPT8fpdPLggw+KCJvNLC8vc/LkSU6cOMGrr74aESvXxsYGMzMzvPHGG0xOTmIymYR1eLPlJxgM0tfXx/z8PFNTUzGXGuRCNjY2GB8fJyMjA1mWMRqNJCcn84d/+IcsLCywsrKCzWYjJSWFrKwsEhISWF9fZ3BwkJdeeokzZ86IzVc0ohy7T05O0traSlxcHHa7na6uLnp6ekQpq2hgW5UfxaM/OTmZ4uJiDAYD09PTtLS00NjYyOLiYtQO6pWi1WpxOBwUFRVhNBpZXl4WmXZVYodoNS1/FEqtqv7+fubn50lNTSUvL08caa2vr4uQYMWxMi4ujs7OTpqbm3n88cdjzqfgciguLqa6uppdu3aRlpYmHJwHBgZi1s9QKeKpUF5eTklJCUePHr3o/TMzMzQ0NPDee+/R2Ni4Xd3cghLa3tzcTHNzc0T6EAmUqLTZ2Vn8fj96vR6r1XrR6DslMGNpaYm+vj6ee+45hoeHoz5CWMnS3dXVRU5ODhkZGXR3d9PW1sbJkyej5h24rcqPRqOhpqaGsrIyLBYLPT09NDY20tXVxcLCwg2j+NhsNvbt20dFRYWIFlJRiQSK0v3MM8+IQpBKPpjNUTZKRN7i4iKrq6s3lLXnQvR6PVVVVciyzJkzZ3jsscd45ZVXosIUfy0YGBjA7XZz1113XdSxWTkavRGV22jH7/fzzjvvMDExwfr6Ovv376eyshK73f6BsWpvb6e/v5/vf//7TExMxJR1cnR0lMcee4znnnsOs9nM8vIyPp8vagJDIALHXoFAgOXlZWEG6+rqYmlpKeq12StFSa+/srJy0bNsFZXtQNk9RiJ9fDSiVAo3m82i9EFHRwf9/f2R7to1Qwlxj6WyOTcLSp3B8fFxzpw5g9FoxO/3bwkKUWhtbWVgYIC2traYC5BZW1tjamqKqampSHflkmyr8hMM9EQIFAAAAZpJREFUBnn77bc5fvw4P//5z4XTaKweLVwKj8fDW2+9JUolVFRUXLRoooqKyvYyMDCAwWAgEAjQ1tbGn//5n8d8PjGV2GNycpIXXniBl19+WZwMXJiDSPEtVOfn9WHbLT+KwnOjWXo2EwqFWF9f59y5czz11FO4XC40Gg3Nzc1MTk5GunsqKjctMzMzSJLEj370I/r6+mLWz0cltlECJ260jX8sIV1J1IokSdEd4vIRyLL8oek9Y10+4LQsy7s+7IZYl/GjxhBUGWMB9VmMfRnVeRrmRpcx1uXjEs+i6omroqKioqKiclOhKj8qKioqKioqNxVX6vMzCwxfj45sAzmXcU8sywc3voyXIx+oMkY7N/o8hRtfRnWe/pYbXcZYlg8uIeMV+fyoqKioqKioqMQ66rGXioqKioqKyk2FqvyoqKioqKio3FSoyo+KioqKiorKTYWq/KioqKioqKjcVKjKj4qKioqKispNhar8qKioqKioqNxUqMqPioqKioqKyk2FqvyoqKioqKio3FSoyo+KioqKiorKTcX/D8OmA6/XB8F7AAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## So sánh các iteration khác nhau"
      ],
      "metadata": {
        "id": "zZ7gQGpO8OMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cost = []\n",
        "acc = []\n",
        "num_iter = []\n",
        "for i in cost_dict:\n",
        "  num_iter.append(i)\n",
        "  cost.append(cost_dict[i])\n",
        "for i in tests_dict:\n",
        "  acc.append(tests_dict[i])"
      ],
      "metadata": {
        "id": "lCoRz288cvuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"iteration = %d: loss = %0.3f, accuracy = %0.3f%s\" %(10000, cost_dict['10000'], tests_dict['10000'], '%'))\n",
        "print(\"iteration = %d: loss = %0.3f, accuracy = %0.3f%s\" %(20000, cost_dict['20000'], tests_dict['20000'], '%'))\n",
        "print(\"iteration = %d: loss = %0.3f, accuracy = %0.3f%s\" %(100000, cost_dict['100000'], tests_dict['100000'], '%'))\n",
        "print(\"iteration = %d: loss = %0.3f, accuracy = %0.3f%s\" %(200000, cost_dict['200000'], tests_dict['200000'], '%'))\n",
        "\n",
        "plt.plot(cost, label='loss_value')\n",
        "plt.plot(acc, label='test_accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "gMFSDSApOl61",
        "outputId": "cdf02022-1225-4c97-f879-9b00e0f0ff76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration = 10000: loss = 0.288, accuracy = 0.917%\n",
            "iteration = 20000: loss = 0.212, accuracy = 0.936%\n",
            "iteration = 100000: loss = 0.105, accuracy = 0.961%\n",
            "iteration = 200000: loss = 0.057, accuracy = 0.975%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f35fd6c78d0>"
            ]
          },
          "metadata": {},
          "execution_count": 107
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "21520064 Neural_network 3 layer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}